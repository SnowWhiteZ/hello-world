#!/usr/bin/python3
# coding: utf-8

from keras_bert import load_trained_model_from_checkpoint, Tokenizer
import codecs
import time
import json
import numpy as np
import pandas as pd
from random import choice
from keras_bert import load_trained_model_from_checkpoint, Tokenizer
from keras_bert.layers.embedding import TokenEmbedding
from keras_pos_embd.pos_embd import PositionEmbedding
from keras_layer_normalization.layer_normalization import LayerNormalization
from keras_multi_head.multi_head_attention import MultiHeadAttention
from keras_position_wise_feed_forward.feed_forward import FeedForward
from keras_bert.bert import gelu_tensorflow


import re, os
import codecs

config_path = '/home/gswyhq/data/chinese_L-12_H-768_A-12/bert_config.json'
checkpoint_path = '/home/gswyhq/data/chinese_L-12_H-768_A-12/bert_model.ckpt'
dict_path = '/home/gswyhq/data/chinese_L-12_H-768_A-12/vocab.txt'

maxlen = 100
token_dict = {}
with codecs.open(dict_path, 'r', 'utf8') as reader:
    for line in reader:
        token = line.strip()
        token_dict[token] = len(token_dict)

class OurTokenizer(Tokenizer):
    """
    Tokenizer有自己的_tokenize方法，这里重写了这个方法，是要保证tokenize之后的结果，跟原来的字符串长度等长（如果算上两个标记，那么就是等长再加2）。
    Tokenizer自带的_tokenize会自动去掉空格，然后有些字符会粘在一块输出，
    导致tokenize之后的列表不等于原来字符串的长度了，这样如果做序列标注的任务会很麻烦。
    而为了避免这种麻烦，还是自己重写一遍好了～主要就是用[unused1]来表示空格类字符，而其余的不在列表的字符用[UNK]表示，
    其中[unused*]这些标记是未经训练的（随即初始化），
    是Bert预留出来用来增量添加词汇的标记，所以我们可以用它们来指代任何新字符。
    """
    def _tokenize(self, text):
        R = []
        for c in text:
            if c in self._token_dict:
                R.append(c)
            elif self._is_space(c):
                R.append('[unused1]') # space类用未经训练的[unused1]表示
            else:
                R.append('[UNK]') # 剩余的字符是[UNK]
        return R

tokenizer = OurTokenizer(token_dict)
cut_words = tokenizer.tokenize(u'今天天气不错')
print(cut_words)

# ['[CLS]', '今', '天', '天', '气', '不', '错', '[SEP]']
# Tokenizer的输出结果:
# 默认情况下，分词后句子首位会分别加上[CLS]和[SEP]标记，其中[CLS]位置对应的输出向量是能代表整句的句向量（反正Bert是这样设计的），
# 而[SEP]则是句间的分隔符，其余部分则是单字输出（对于中文来说）。

# 语料来源：https://github.com/zhuanxuhit/nd101/tree/master/1.Intro_to_Deep_Learning/3.How_to_Do_Sentiment_Analysis/data
neg = pd.read_excel('/home/gswyhq/data/neg.xls', header=None)
pos = pd.read_excel('/home/gswyhq/data/pos.xls', header=None)

data = []

for d in neg[0]:
    data.append((d, 0))

for d in pos[0]:
    data.append((d, 1))



# 按照9:1的比例划分训练集和验证集
random_order = range(len(data))
np.random.shuffle(list(random_order))
train_data = [data[j] for i, j in enumerate(random_order) if i % 10 != 0]
valid_data = [data[j] for i, j in enumerate(random_order) if i % 10 == 0]

# len(train_data)
# Out[4]: 18994
# len(valid_data)
# Out[5]: 2111

# 全面数据训练时间太久，为了演示减少数据量
# train_data = train_data[:2000] + train_data[-2000:]
# valid_data = valid_data[:200] + valid_data[-200:]

def seq_padding(X, padding=0):
    L = [len(x) for x in X]
    ML = max(L)
    return np.array([
        np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X
    ])


class data_generator:
    def __init__(self, data, batch_size=24):
        self.data = data
        self.batch_size = batch_size
        self.steps = len(self.data) // self.batch_size
        if len(self.data) % self.batch_size != 0:
            self.steps += 1
    def __len__(self):
        return self.steps
    def __iter__(self):
        while True:
            idxs = range(len(self.data))
            np.random.shuffle(list(idxs))
            X1, X2, Y = [], [], []
            for i in idxs:
                d = self.data[i]
                text = d[0][:maxlen]
                x1, x2 = tokenizer.encode(first=text)
                y = d[1]
                X1.append(x1)
                X2.append(x2)
                Y.append([y])
                if len(X1) == self.batch_size or i == idxs[-1]:
                    X1 = seq_padding(X1)
                    X2 = seq_padding(X2)
                    Y = seq_padding(Y)
                    yield [X1, X2], Y
                    [X1, X2, Y] = [], [], []


from keras.layers import *
from keras.models import Model, load_model
import keras.backend as K
from keras.optimizers import Adam


bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, seq_len=None)

for l in bert_model.layers:
    l.trainable = True

x1_in = Input(shape=(None,))
x2_in = Input(shape=(None,))

x = bert_model([x1_in, x2_in])
x = Lambda(lambda x: x[:, 0])(x)
p = Dense(1, activation='sigmoid')(x)

model = Model([x1_in, x2_in], p)
model.compile(
    loss='binary_crossentropy',
    optimizer=Adam(1e-5), # 用足够小的学习率
    metrics=['accuracy']
)
model.summary()

'''
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, None)         0
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, None)         0
__________________________________________________________________________________________________
model_2 (Model)                 (None, None, 768)    101677056   input_1[0][0]
                                                                 input_2[0][0]
__________________________________________________________________________________________________
lambda_1 (Lambda)               (None, 768)          0           model_2[1][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            769         lambda_1[0][0]
==================================================================================================
Total params: 101,677,825
Trainable params: 101,677,825
Non-trainable params: 0
__________________________________________________________________________________________________
'''

train_D = data_generator(train_data)
valid_D = data_generator(valid_data)

start_time = time.time()
model.fit_generator(
    train_D.__iter__(),
    steps_per_epoch=len(train_D),
    epochs=1,
    validation_data=valid_D.__iter__(),
    validation_steps=len(valid_D)
)
model.save('model.h5')
print('总耗时：{}'.format(time.time() - start_time))

def predict(data, model_file='/home/gswyhq/hello-world/bert/model.h5'):
    model = load_model(model_file, custom_objects={'TokenEmbedding':TokenEmbedding,
                                                 'PositionEmbedding': PositionEmbedding,
                                                 'LayerNormalization': LayerNormalization,
                                                 'MultiHeadAttention': MultiHeadAttention,
                                                 'FeedForward': FeedForward,
                                                 'gelu_tensorflow': gelu_tensorflow})
    X1, X2 = [], []
    for text in data:
        text = text[:maxlen]
        x1, x2 = tokenizer.encode(first=text)

        X1.append(x1)
        X2.append(x2)
    X1 = seq_padding(X1)
    X2 = seq_padding(X2)
    p = model.predict([X1, X2])
    return p

def main():
    data = ['我是带孩子去的.电视画面一打开就是不堪入目的镜头.怕孩子乱摁,只有把遥控器藏起来.作为五星酒店怎么可以象街头旅馆? 酒店应该反思一下.',
            '至少我没觉得好看，根本就不吸引我，没什么可笑得，有些词很深辟。书的印刷很不错，内容没介绍的好。',
            '质量不好，差评',
            '点赞，好评'
            ]
    # predict(data)


if __name__ == '__main__':
    main()

# /usr/bin/python3.6 /home/gswyhq/hello-world/bert/微调bert进行文本分类.py
# Using TensorFlow backend.
# ['[CLS]', '今', '天', '天', '气', '不', '错', '[SEP]']
# WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
# Instructions for updating:
# Colocations handled automatically by placer.
# WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
# Instructions for updating:
# Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
# 2019-07-21 08:36:45.736379: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2294685000 Hz
# 2019-07-21 08:36:45.736784: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2e9f180 executing computations on platform Host. Devices:
# 2019-07-21 08:36:45.736819: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
# __________________________________________________________________________________________________
# Layer (type)                    Output Shape         Param #     Connected to
# ==================================================================================================
# input_1 (InputLayer)            (None, None)         0
# __________________________________________________________________________________________________
# input_2 (InputLayer)            (None, None)         0
# __________________________________________________________________________________________________
# model_2 (Model)                 (None, None, 768)    101677056   input_1[0][0]
#                                                                  input_2[0][0]
# __________________________________________________________________________________________________
# lambda_1 (Lambda)               (None, 768)          0           model_2[1][0]
# __________________________________________________________________________________________________
# dense_1 (Dense)                 (None, 1)            769         lambda_1[0][0]
# ==================================================================================================
# Total params: 101,677,825
# Trainable params: 101,677,825
# Non-trainable params: 0
# __________________________________________________________________________________________________
# WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
# Instructions for updating:
# Use tf.cast instead.
# WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
# Instructions for updating:
# Deprecated in favor of operator or tf.math.divide.
# Epoch 1/1
#
#   1/209 [..............................] - ETA: 3:10:34 - loss: 0.2381 - acc: 0.9583
#   2/209 [..............................] - ETA: 2:33:36 - loss: 0.1917 - acc: 0.9792
#   3/209 [..............................] - ETA: 2:26:06 - loss: 0.1446 - acc: 0.9861
#   4/209 [..............................] - ETA: 2:19:53 - loss: 0.1120 - acc: 0.9896
#   5/209 [..............................] - ETA: 2:19:06 - loss: 0.0909 - acc: 0.9917
#   6/209 [..............................] - ETA: 2:18:35 - loss: 0.0762 - acc: 0.9931
#   7/209 [>.............................] - ETA: 2:27:22 - loss: 0.0656 - acc: 0.9940
#   8/209 [>.............................] - ETA: 2:30:57 - loss: 0.0576 - acc: 0.9948
#   9/209 [>.............................] - ETA: 2:35:11 - loss: 0.0513 - acc: 0.9954
#  10/209 [>.............................] - ETA: 2:35:28 - loss: 0.0463 - acc: 0.9958
#  11/209 [>.............................] - ETA: 2:34:21 - loss: 0.0421 - acc: 0.9962
#  12/209 [>.............................] - ETA: 2:35:05 - loss: 0.0386 - acc: 0.9965
#  13/209 [>.............................] - ETA: 2:36:41 - loss: 0.0357 - acc: 0.9968
#  14/209 [=>............................] - ETA: 2:36:51 - loss: 0.0332 - acc: 0.9970
#  15/209 [=>............................] - ETA: 2:41:36 - loss: 0.0310 - acc: 0.9972
#  16/209 [=>............................] - ETA: 2:43:06 - loss: 0.0291 - acc: 0.9974
#  17/209 [=>............................] - ETA: 2:44:47 - loss: 0.0274 - acc: 0.9975
#  18/209 [=>............................] - ETA: 2:46:41 - loss: 0.0258 - acc: 0.9977
#  19/209 [=>............................] - ETA: 2:47:26 - loss: 0.0245 - acc: 0.9978
#  20/209 [=>............................] - ETA: 2:48:53 - loss: 0.0233 - acc: 0.9979
#  21/209 [==>...........................] - ETA: 2:49:47 - loss: 0.0222 - acc: 0.9980
#  22/209 [==>...........................] - ETA: 2:50:40 - loss: 0.0212 - acc: 0.9981
#  23/209 [==>...........................] - ETA: 2:51:32 - loss: 0.0202 - acc: 0.9982
#  24/209 [==>...........................] - ETA: 2:52:10 - loss: 0.0194 - acc: 0.9983
#  25/209 [==>...........................] - ETA: 2:52:29 - loss: 0.0186 - acc: 0.9983
#  26/209 [==>...........................] - ETA: 2:52:01 - loss: 0.0179 - acc: 0.9984
#  27/209 [==>...........................] - ETA: 2:52:19 - loss: 0.0172 - acc: 0.9985
#  28/209 [===>..........................] - ETA: 2:52:19 - loss: 0.0166 - acc: 0.9985
#  29/209 [===>..........................] - ETA: 2:52:26 - loss: 0.0161 - acc: 0.9986
#  30/209 [===>..........................] - ETA: 2:52:09 - loss: 0.0155 - acc: 0.9986
#  31/209 [===>..........................] - ETA: 2:52:55 - loss: 0.0150 - acc: 0.9987
#  32/209 [===>..........................] - ETA: 2:52:36 - loss: 0.0146 - acc: 0.9987
#  33/209 [===>..........................] - ETA: 2:50:25 - loss: 0.0141 - acc: 0.9987
#  34/209 [===>..........................] - ETA: 2:49:02 - loss: 0.0137 - acc: 0.9988
#  35/209 [====>.........................] - ETA: 2:47:45 - loss: 0.0133 - acc: 0.9988
#  36/209 [====>.........................] - ETA: 2:46:26 - loss: 0.0129 - acc: 0.9988
#  37/209 [====>.........................] - ETA: 2:44:59 - loss: 0.0126 - acc: 0.9989
#  38/209 [====>.........................] - ETA: 2:43:53 - loss: 0.0123 - acc: 0.9989
#  39/209 [====>.........................] - ETA: 2:42:13 - loss: 0.0120 - acc: 0.9989
#  40/209 [====>.........................] - ETA: 2:40:55 - loss: 0.0117 - acc: 0.9990
#  41/209 [====>.........................] - ETA: 2:39:20 - loss: 0.0114 - acc: 0.9990
#  42/209 [=====>........................] - ETA: 2:37:01 - loss: 0.0111 - acc: 0.9990
#  43/209 [=====>........................] - ETA: 2:35:44 - loss: 0.0108 - acc: 0.9990
#  44/209 [=====>........................] - ETA: 2:34:39 - loss: 0.0106 - acc: 0.9991
#  45/209 [=====>........................] - ETA: 2:33:37 - loss: 0.0104 - acc: 0.9991
#  46/209 [=====>........................] - ETA: 2:32:07 - loss: 0.0101 - acc: 0.9991
#  47/209 [=====>........................] - ETA: 2:31:02 - loss: 0.0099 - acc: 0.9991
#  48/209 [=====>........................] - ETA: 2:30:10 - loss: 0.0097 - acc: 0.9991
#  49/209 [======>.......................] - ETA: 2:28:54 - loss: 0.0095 - acc: 0.9991
#  50/209 [======>.......................] - ETA: 2:27:51 - loss: 0.0093 - acc: 0.9992
#  51/209 [======>.......................] - ETA: 2:27:04 - loss: 0.0091 - acc: 0.9992
#  52/209 [======>.......................] - ETA: 2:25:50 - loss: 0.0090 - acc: 0.9992
#  53/209 [======>.......................] - ETA: 2:24:53 - loss: 0.0088 - acc: 0.9992
#  54/209 [======>.......................] - ETA: 2:23:55 - loss: 0.0086 - acc: 0.9992
#  55/209 [======>.......................] - ETA: 2:22:55 - loss: 0.0085 - acc: 0.9992
#  56/209 [=======>......................] - ETA: 2:21:55 - loss: 0.0083 - acc: 0.9993
#  57/209 [=======>......................] - ETA: 2:20:58 - loss: 0.0082 - acc: 0.9993
#  58/209 [=======>......................] - ETA: 2:19:53 - loss: 0.0080 - acc: 0.9993
#  59/209 [=======>......................] - ETA: 2:18:56 - loss: 0.0079 - acc: 0.9993
#  60/209 [=======>......................] - ETA: 2:17:20 - loss: 0.0078 - acc: 0.9993
#  61/209 [=======>......................] - ETA: 2:16:21 - loss: 0.0077 - acc: 0.9993
#  62/209 [=======>......................] - ETA: 2:15:24 - loss: 0.0075 - acc: 0.9993
#  63/209 [========>.....................] - ETA: 2:14:26 - loss: 0.0074 - acc: 0.9993
#  64/209 [========>.....................] - ETA: 2:13:20 - loss: 0.0073 - acc: 0.9993
#  65/209 [========>.....................] - ETA: 2:12:21 - loss: 0.0072 - acc: 0.9994
#  66/209 [========>.....................] - ETA: 2:11:27 - loss: 0.0071 - acc: 0.9994
#  67/209 [========>.....................] - ETA: 2:10:28 - loss: 0.0070 - acc: 0.9994
#  68/209 [========>.....................] - ETA: 2:09:30 - loss: 0.0069 - acc: 0.9994
#  69/209 [========>.....................] - ETA: 2:08:09 - loss: 0.0068 - acc: 0.9994
#  70/209 [=========>....................] - ETA: 2:07:17 - loss: 0.0067 - acc: 0.9994
#  71/209 [=========>....................] - ETA: 2:06:26 - loss: 0.0066 - acc: 0.9994
#  72/209 [=========>....................] - ETA: 2:05:26 - loss: 0.0065 - acc: 0.9994
#  73/209 [=========>....................] - ETA: 2:04:29 - loss: 0.0064 - acc: 0.9994
#  74/209 [=========>....................] - ETA: 2:03:37 - loss: 0.0063 - acc: 0.9994
#  75/209 [=========>....................] - ETA: 2:02:38 - loss: 0.0062 - acc: 0.9994
#  76/209 [=========>....................] - ETA: 2:01:43 - loss: 0.0062 - acc: 0.9995
#  77/209 [==========>...................] - ETA: 2:00:49 - loss: 0.0061 - acc: 0.9995
#  78/209 [==========>...................] - ETA: 1:59:56 - loss: 0.0060 - acc: 0.9995
#  79/209 [==========>...................] - ETA: 1:58:56 - loss: 0.0059 - acc: 0.9995
#  80/209 [==========>...................] - ETA: 1:58:02 - loss: 0.0058 - acc: 0.9995
#  81/209 [==========>...................] - ETA: 1:57:03 - loss: 0.0058 - acc: 0.9995
#  82/209 [==========>...................] - ETA: 1:56:03 - loss: 0.0057 - acc: 0.9995
#  83/209 [==========>...................] - ETA: 1:55:10 - loss: 0.0056 - acc: 0.9995
#  84/209 [===========>..................] - ETA: 1:54:12 - loss: 0.0056 - acc: 0.9995
#  85/209 [===========>..................] - ETA: 1:53:13 - loss: 0.0055 - acc: 0.9995
#  86/209 [===========>..................] - ETA: 1:52:20 - loss: 0.0054 - acc: 0.9995
#  87/209 [===========>..................] - ETA: 1:51:22 - loss: 0.0054 - acc: 0.9995
#  88/209 [===========>..................] - ETA: 1:50:29 - loss: 0.0053 - acc: 0.9995
#  89/209 [===========>..................] - ETA: 1:49:35 - loss: 0.0053 - acc: 0.9995
#  90/209 [===========>..................] - ETA: 1:48:39 - loss: 0.0052 - acc: 0.9995
#  91/209 [============>.................] - ETA: 1:47:39 - loss: 0.0051 - acc: 0.9995
#  92/209 [============>.................] - ETA: 1:46:44 - loss: 0.0051 - acc: 0.9995
#  93/209 [============>.................] - ETA: 1:45:51 - loss: 0.0050 - acc: 0.9996
#  94/209 [============>.................] - ETA: 1:44:52 - loss: 0.0050 - acc: 0.9996
#  95/209 [============>.................] - ETA: 1:43:55 - loss: 0.0049 - acc: 0.9996
#  96/209 [============>.................] - ETA: 1:43:02 - loss: 0.0049 - acc: 0.9996
#  97/209 [============>.................] - ETA: 1:42:06 - loss: 0.0048 - acc: 0.9996
#  98/209 [=============>................] - ETA: 1:41:11 - loss: 0.0048 - acc: 0.9996
#  99/209 [=============>................] - ETA: 1:40:18 - loss: 0.0047 - acc: 0.9996
# 100/209 [=============>................] - ETA: 1:39:19 - loss: 0.0047 - acc: 0.9996
# 101/209 [=============>................] - ETA: 1:38:20 - loss: 0.0046 - acc: 0.9996
# 102/209 [=============>................] - ETA: 1:37:25 - loss: 0.0046 - acc: 0.9996
# 103/209 [=============>................] - ETA: 1:36:29 - loss: 0.0045 - acc: 0.9996
# 104/209 [=============>................] - ETA: 1:35:34 - loss: 0.0045 - acc: 0.9996
# 105/209 [==============>...............] - ETA: 1:34:38 - loss: 0.0045 - acc: 0.9996
# 106/209 [==============>...............] - ETA: 1:33:45 - loss: 0.0044 - acc: 0.9996
# 107/209 [==============>...............] - ETA: 1:32:46 - loss: 0.0044 - acc: 0.9996
# 108/209 [==============>...............] - ETA: 1:31:52 - loss: 0.0043 - acc: 0.9996
# 109/209 [==============>...............] - ETA: 1:30:56 - loss: 0.0043 - acc: 0.9996
# 110/209 [==============>...............] - ETA: 1:30:00 - loss: 0.0043 - acc: 0.9996
# 111/209 [==============>...............] - ETA: 1:29:06 - loss: 0.0042 - acc: 0.9996
# 112/209 [===============>..............] - ETA: 1:28:10 - loss: 0.0042 - acc: 0.9996
# 113/209 [===============>..............] - ETA: 1:27:18 - loss: 0.0041 - acc: 0.9996
# 114/209 [===============>..............] - ETA: 1:26:22 - loss: 0.0041 - acc: 0.9996
# 115/209 [===============>..............] - ETA: 1:25:27 - loss: 0.0041 - acc: 0.9996
# 116/209 [===============>..............] - ETA: 1:24:32 - loss: 0.0040 - acc: 0.9996
# 117/209 [===============>..............] - ETA: 1:23:37 - loss: 0.0040 - acc: 0.9996
# 118/209 [===============>..............] - ETA: 1:22:43 - loss: 0.0040 - acc: 0.9996
# 119/209 [================>.............] - ETA: 1:21:52 - loss: 0.0039 - acc: 0.9996
# 120/209 [================>.............] - ETA: 1:20:55 - loss: 0.0039 - acc: 0.9997
# 121/209 [================>.............] - ETA: 1:20:01 - loss: 0.0039 - acc: 0.9997
# 122/209 [================>.............] - ETA: 1:19:07 - loss: 0.0038 - acc: 0.9997
# 123/209 [================>.............] - ETA: 1:18:12 - loss: 0.0038 - acc: 0.9997
# 124/209 [================>.............] - ETA: 1:17:19 - loss: 0.0038 - acc: 0.9997
# 125/209 [================>.............] - ETA: 1:16:24 - loss: 0.0037 - acc: 0.9997
# 126/209 [=================>............] - ETA: 1:15:28 - loss: 0.0037 - acc: 0.9997
# 127/209 [=================>............] - ETA: 1:14:34 - loss: 0.0037 - acc: 0.9997
# 128/209 [=================>............] - ETA: 1:13:41 - loss: 0.0037 - acc: 0.9997
# 129/209 [=================>............] - ETA: 1:12:46 - loss: 0.0036 - acc: 0.9997
# 130/209 [=================>............] - ETA: 1:11:53 - loss: 0.0036 - acc: 0.9997
# 131/209 [=================>............] - ETA: 1:10:58 - loss: 0.0036 - acc: 0.9997
# 132/209 [=================>............] - ETA: 1:10:03 - loss: 0.0035 - acc: 0.9997
# 133/209 [==================>...........] - ETA: 1:09:07 - loss: 0.0035 - acc: 0.9997
# 134/209 [==================>...........] - ETA: 1:08:13 - loss: 0.0035 - acc: 0.9997
# 135/209 [==================>...........] - ETA: 1:07:21 - loss: 0.0035 - acc: 0.9997
# 136/209 [==================>...........] - ETA: 1:06:24 - loss: 0.0034 - acc: 0.9997
# 137/209 [==================>...........] - ETA: 1:05:31 - loss: 0.0034 - acc: 0.9997
# 138/209 [==================>...........] - ETA: 1:04:37 - loss: 0.0034 - acc: 0.9997
# 139/209 [==================>...........] - ETA: 1:03:42 - loss: 0.0034 - acc: 0.9997
# 140/209 [===================>..........] - ETA: 1:02:47 - loss: 0.0033 - acc: 0.9997
# 141/209 [===================>..........] - ETA: 1:01:57 - loss: 0.0033 - acc: 0.9997
# 142/209 [===================>..........] - ETA: 1:01:10 - loss: 0.0033 - acc: 0.9997
# 143/209 [===================>..........] - ETA: 1:00:23 - loss: 0.0033 - acc: 0.9997
# 144/209 [===================>..........] - ETA: 59:38 - loss: 0.0033 - acc: 0.9997
# 145/209 [===================>..........] - ETA: 58:51 - loss: 0.0032 - acc: 0.9997
# 146/209 [===================>..........] - ETA: 58:04 - loss: 0.0032 - acc: 0.9997
# 147/209 [====================>.........] - ETA: 57:17 - loss: 0.0032 - acc: 0.9997
# 148/209 [====================>.........] - ETA: 56:29 - loss: 0.0032 - acc: 0.9997
# 149/209 [====================>.........] - ETA: 55:42 - loss: 0.0031 - acc: 0.9997
# 150/209 [====================>.........] - ETA: 54:52 - loss: 0.0031 - acc: 0.9997
# 151/209 [====================>.........] - ETA: 54:03 - loss: 0.0031 - acc: 0.9997
# 152/209 [====================>.........] - ETA: 53:06 - loss: 0.0031 - acc: 0.9997
# 153/209 [====================>.........] - ETA: 52:11 - loss: 0.0031 - acc: 0.9997
# 154/209 [=====================>........] - ETA: 51:13 - loss: 0.0030 - acc: 0.9997
# 155/209 [=====================>........] - ETA: 50:15 - loss: 0.0030 - acc: 0.9997
# 156/209 [=====================>........] - ETA: 49:19 - loss: 0.0030 - acc: 0.9997
# 157/209 [=====================>........] - ETA: 48:22 - loss: 0.0030 - acc: 0.9997
# 158/209 [=====================>........] - ETA: 47:25 - loss: 0.0030 - acc: 0.9997
# 159/209 [=====================>........] - ETA: 46:27 - loss: 0.0029 - acc: 0.9997
# 160/209 [=====================>........] - ETA: 45:43 - loss: 0.0029 - acc: 0.9997
# 161/209 [======================>.......] - ETA: 44:46 - loss: 0.0029 - acc: 0.9997
# 162/209 [======================>.......] - ETA: 43:48 - loss: 0.0029 - acc: 0.9997
# 163/209 [======================>.......] - ETA: 42:51 - loss: 0.0029 - acc: 0.9997
# 164/209 [======================>.......] - ETA: 41:55 - loss: 0.0029 - acc: 0.9997
# 165/209 [======================>.......] - ETA: 40:57 - loss: 0.0028 - acc: 0.9997
# 166/209 [======================>.......] - ETA: 39:59 - loss: 0.0028 - acc: 0.9997
# 167/209 [======================>.......] - ETA: 39:03 - loss: 0.0028 - acc: 0.9998
# 168/209 [=======================>......] - ETA: 38:05 - loss: 0.0028 - acc: 0.9998
# 169/209 [=======================>......] - ETA: 37:10 - loss: 0.0028 - acc: 0.9998
# 170/209 [=======================>......] - ETA: 36:13 - loss: 0.0028 - acc: 0.9998
# 171/209 [=======================>......] - ETA: 35:16 - loss: 0.0027 - acc: 0.9998
# 172/209 [=======================>......] - ETA: 34:20 - loss: 0.0027 - acc: 0.9998
# 173/209 [=======================>......] - ETA: 33:24 - loss: 0.0027 - acc: 0.9998
# 174/209 [=======================>......] - ETA: 32:28 - loss: 0.0027 - acc: 0.9998
# 175/209 [========================>.....] - ETA: 31:31 - loss: 0.0027 - acc: 0.9998
# 176/209 [========================>.....] - ETA: 30:34 - loss: 0.0027 - acc: 0.9998
# 177/209 [========================>.....] - ETA: 29:38 - loss: 0.0027 - acc: 0.9998
# 178/209 [========================>.....] - ETA: 28:42 - loss: 0.0026 - acc: 0.9998
# 179/209 [========================>.....] - ETA: 27:47 - loss: 0.0026 - acc: 0.9998
# 180/209 [========================>.....] - ETA: 26:51 - loss: 0.0026 - acc: 0.9998
# 181/209 [========================>.....] - ETA: 25:55 - loss: 0.0026 - acc: 0.9998
# 182/209 [=========================>....] - ETA: 24:59 - loss: 0.0026 - acc: 0.9998
# 183/209 [=========================>....] - ETA: 24:03 - loss: 0.0026 - acc: 0.9998
# 184/209 [=========================>....] - ETA: 23:08 - loss: 0.0026 - acc: 0.9998
# 185/209 [=========================>....] - ETA: 22:14 - loss: 0.0025 - acc: 0.9998
# 186/209 [=========================>....] - ETA: 21:19 - loss: 0.0025 - acc: 0.9998
# 187/209 [=========================>....] - ETA: 20:26 - loss: 0.0025 - acc: 0.9998
# 188/209 [=========================>....] - ETA: 19:31 - loss: 0.0025 - acc: 0.9998
# 189/209 [==========================>...] - ETA: 18:35 - loss: 0.0025 - acc: 0.9998
# 190/209 [==========================>...] - ETA: 17:39 - loss: 0.0025 - acc: 0.9998
# 191/209 [==========================>...] - ETA: 16:43 - loss: 0.0025 - acc: 0.9998
# 192/209 [==========================>...] - ETA: 15:47 - loss: 0.0024 - acc: 0.9998
# 193/209 [==========================>...] - ETA: 14:51 - loss: 0.0024 - acc: 0.9998
# 194/209 [==========================>...] - ETA: 13:55 - loss: 0.0024 - acc: 0.9998
# 195/209 [==========================>...] - ETA: 12:59 - loss: 0.0024 - acc: 0.9998
# 196/209 [===========================>..] - ETA: 12:03 - loss: 0.0024 - acc: 0.9998
# 197/209 [===========================>..] - ETA: 11:07 - loss: 0.0024 - acc: 0.9998
# 198/209 [===========================>..] - ETA: 10:11 - loss: 0.0024 - acc: 0.9998
# 199/209 [===========================>..] - ETA: 9:15 - loss: 0.0024 - acc: 0.9998
# 200/209 [===========================>..] - ETA: 8:20 - loss: 0.0023 - acc: 0.9998
# 201/209 [===========================>..] - ETA: 7:25 - loss: 0.0023 - acc: 0.9998
# 202/209 [===========================>..] - ETA: 6:29 - loss: 0.0023 - acc: 0.9998
# 203/209 [============================>.] - ETA: 5:34 - loss: 0.0023 - acc: 0.9998
# 204/209 [============================>.] - ETA: 4:39 - loss: 0.0023 - acc: 0.9998
# 205/209 [============================>.] - ETA: 3:43 - loss: 0.0023 - acc: 0.9998
# 206/209 [============================>.] - ETA: 2:47 - loss: 0.0023 - acc: 0.9998
# 207/209 [============================>.] - ETA: 1:51 - loss: 0.0023 - acc: 0.9998
# 208/209 [============================>.] - ETA: 55s - loss: 0.0023 - acc: 0.9998
# 209/209 [==============================] - 12191s 58s/step - loss: 0.0022 - acc: 0.9998 - val_loss: 1.4075e-05 - val_acc: 1.0000
# 总耗时：12274.743566989899
#
# Process finished with exit code 0
