
Name:LDC2013T21.tgz： https://wakespace.lib.wfu.edu/bitstream/handle/10339/39379/LDC2013T21.tgz?sequence=1

toutiao_data.csv: 
    今日头条中文新闻（文本）分类数据集 
    每行为一条数据，以!分割的个字段，从前往后分别是 新闻ID，分类code（见下文），分类名称（见下文），新闻字符串（仅含标题），新闻关键词。
    来源: https://github.com/mattzheng/LangueOne/toutiao_data.rar; https://github.com/fateleak/toutiao-text-classfication-dataset.git

GoogleNews-vectors-negative300.bin.gz
gunzip -c GoogleNews-vectors-negative300.bin.gz > GoogleNews-vectors-negative300.bin
    https://github.com/3Top/word2vec-api
    https://pan.baidu.com/s/1kTCQqft

/home/gswyhq/data/kaggle_dogs-vs-cats
猫狗分类数据集不包含在 Keras 中。它由 Kaggle 在 2013 年末公开并作为一项计算视觉竞赛的一部分,当时卷积神经网络还不是主流算法。
# 你可以从 https://www.kaggle.com/c/dogs-vs-cats/data 下载原始数据集
#  数据集 链接：https://pan.baidu.com/s/13hw4LK8ihR6-6-8mpjLKDA 密码：dmp4

icwb2-data.rar
    http://sighan.cs.uchicago.edu/bakeoff2005/data/icwb2-data.rar

SogouC.reduced.zip
    https://pan.baidu.com/s/1pLrORJt

蛋白质功能预测：
https://github.com/tbepler/protein-sequence-embedding-iclr2019
- [SCOPe data](http://bergerlab-downloads.csail.mit.edu/bepler-protein-sequence-embeddings-from-structure-iclr2019/scope.tar.gz)
- [Pfam data](http://bergerlab-downloads.csail.mit.edu/bepler-protein-sequence-embeddings-from-structure-iclr2019/pfam.tar.gz)
- [Protein secondary structure data](http://bergerlab-downloads.csail.mit.edu/bepler-protein-sequence-embeddings-from-structure-iclr2019/secstr.tar.gz)
- [Transmembrane data](http://bergerlab-downloads.csail.mit.edu/bepler-protein-sequence-embeddings-from-structure-iclr2019/transmembrane.tar.gz)
- [CASP12 contact map data](http://bergerlab-downloads.csail.mit.edu/bepler-protein-sequence-embeddings-from-structure-iclr2019/casp12.tar.gz)

词性标注@人民日报199801.txt
https://pan.baidu.com/s/1gd6mslt

中华古诗词数据库
最全中华古诗词数据集，唐宋两朝近一万四千古诗人, 接近5.5万首唐诗加26万宋诗. 两宋时期1564位词人，21050首词。
https://github.com/chinese-poetry/chinese-poetry

汉语拆字字典
https://github.com/kfcd/chaizi

中华新华字典数据库。包括歇后语，成语，词语，汉字。
https://github.com/pwxcoo/chinese-xinhua

中文实体情感知识库
刻画人们如何描述某个实体，包含新闻、旅游、餐饮，共计30万对。
https://github.com/rainarch/SentiBridge

# 《TensorFlow+Keras深度学习人工智能实践应用》随书示例
https://pan.baidu.com/s/1c2rXnH2#list/path=%2FMP21710_example.zip
MP21710_example.zip

分词数据集 people-2014.7z：
数据集: https://pan.baidu.com/s/1EtXdhPR0lGF8c7tT8epn6Q 验证码: yj9j

http://183.61.19.162/zdoc/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA(%E7%AC%AC3%E7%89%88).pdf
http://183.61.19.162/zdoc/算法导论(第3版).pdf

七律百科知识图谱
7lore.zip
http://openkg.cn/dataset/7lore
http://openkg1.oss-cn-beijing.aliyuncs.com/30ee798a-3199-4de4-9792-7746bc8891e0/7lore.zip
7Lore_triple.csv

中文百科知识图谱Zhishi.me-提供Dump
zhishimejson.tar.gz
~$ tar -zxvf zhishimejson.tar.gz
zhishime_json/
zhishime_json/sameAs/
zhishime_json/sameAs/2.9_baidubaike_zhwiki_links_zh.zip
zhishime_json/sameAs/2.9_hudongbaike_zhwiki_links_zh.zip
zhishime_json/sameAs/2.9_baidubaike_hudongbaike_links_zh.zip
zhishime_json/sameAs/2.9_zhwiki_baidubaike_links_zh.zip
zhishime_json/sameAs/2.9_zhwiki_hudongbaike_links_zh.zip
zhishime_json/sameAs/2.9_hudongbaike_baidubaike_links_zh.zip
zhishime_json/3.0_zhishi_ontology_zh.zip
zhishime_json/baidubaike/
zhishime_json/baidubaike/3.0_baidubaike_redirects_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_abstracts_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_external_links_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_images_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_infobox_property_definitions_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_internal_links_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_related_pages_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_labels_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_article_categories_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_image_information_zh.zip
zhishime_json/baidubaike/baidubaike_instance_types_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_disambiguations_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_infobox_properties_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_article_links_zh.zip
zhishime_json/baidubaike/3.0_baidubaike_category_labels_zh.zip
zhishime_json/zhwiki/
zhishime_json/zhwiki/2.0_zhwiki_external_links_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_article_links_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_resource_ids_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_revisions_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_abstracts_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_dbpedia_links_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_skos_categories_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_internal_links_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_images_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_aliases_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_infobox_property_definitions_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_labels_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_category_labels_zh.zip
zhishime_json/zhwiki/zhwiki_instance_types_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_redirects_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_disambiguations_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_infobox_properties_zh.zip
zhishime_json/zhwiki/2.0_zhwiki_article_categories_zh.zip
zhishime_json/hudongbaike/
zhishime_json/hudongbaike/3.0_hudongbaike_abstracts_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_article_links_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_category_labels_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_infobox_properties_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_external_links_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_image_information_zh.zip
zhishime_json/hudongbaike/hudongbaike_instance_types_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_internal_links_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_disambiguations_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_labels_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_images_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_related_pages_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_redirects_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_article_categories_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_infobox_property_definitions_zh.zip
zhishime_json/hudongbaike/3.0_hudongbaike_skos_categories_zh.zip
http://openkg1.oss-cn-beijing.aliyuncs.com/d71361be-b741-480f-95ca-e0eae5931877/zhishimejson.tar.gz
http://openkg.cn/dataset/zhishi-me-dump

通用知识图谱（ownthink）
http://openkg.cn/dataset/ownthink
五千万知识图谱数据拿走不谢
链接: https://pan.baidu.com/s/1iATmmV51XwejAAGLsWJ3OA 提取码: fv49 
data.tar.gz
~$ tar -zxvf data.tar.gz
data.txt
~$ md5sum data.txt
c937abff957fdf75d633df14a21b01ea  data.txt
1.4亿中文知识图谱：
ownthink_v2.zip（链接: https://pan.baidu.com/s/1LZjs9Dsta0yD9NH-1y0sAw 提取码: 3hpp ）
unzip -P "https://www.ownthink.com/" ownthink_v2.zip
Archive:  ownthink_v2.zip
  inflating: ownthink_v2.csv
~$ md5sum ownthink_v2.csv
f4c491f051b8af6bc4ab2d68b6b8b82a  ownthink_v2.csv

医疗知识图谱数据
链接: https://pan.baidu.com/s/178DMxv9P-kK7DssofH6URw 提取码: 5x4m 复制这段内容后打开百度网盘手机App，操作更方便哦
解压密码是：www.ownthink.com
Disease.zip

数据下载于：http://openkg.cn/dataset/cndbpedia
/home/gswyhq/data/中文通用百科知识图谱（CN-DBpedia）/baiketriples.zip
$ unzip baiketriples.zip
Archive:  baiketriples.zip
  inflating: baike_triples.txt
CN-DBpedia Dump数据（2015.07）CSV
包含900万+的百科实体以及6600万+的三元组关系。其中摘要信息400万+，标签信息1980万+，infobox信息4100万+


/home/gswyhq/data/中文通用百科知识图谱（CN-DBpedia）/m2e.zip
$ unzip m2e.zip
Archive:  m2e.zip
  inflating: m2e.txt
CN-DBpedia Mention2Entity数据（2015.07）CSV
包含110万+的mention2entity数据

语音合成：
https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2
http://cn-mirror.openslr.org/resources/47/primewords_md_2018_set1.tar.gz

聊天语料：
raw_chat_corpus.zip
存在 https://pan.baidu.com/s/1szmNZQrwh9y994uO8DFL_A 提取码：f2ex 中。

百度中文问答数据集：
https://pan.baidu.com/s/1QUsKcFWZ7Tg1dk_AbldZ1A, 提取码：2dva
WebQA.v1.0.7z

纯净版：
链接: https://pan.baidu.com/s/1pLXEYtd 密码: 6fbf
文件列表：
WebQA.v1.0/readme.txt
WebQA.v1.0/me_test.ann.json （一个问题只配一段材料，材料中有答案）
WebQA.v1.0/me_test.ir.json （一个问题配多段材料，材料可能有也可能没有答案）
WebQA.v1.0/me_train.json （混合的训练语料）
WebQA.v1.0/me_validation.ann.json （一个问题只配一段材料，材料中有答案）
WebQA.v1.0/me_validation.ir.json （一个问题配多段材料，材料可能有也可能没有答案）

mnist.npz:
下载链接：https://pan.baidu.com/s/1jH6uFFC 密码: dw3d
测试keras的代码时显示需要下载'https://s3.amazonaws.com/img-datasets/mnist.npz'，下载了多次都无法访问该地址，致使测试搁置。翻墙下载下来，需要的可以从这里下载解压后放在~/.keras/datases/里面，然后可以运行。

aclImdb.zip
http://s3.amazonaws.com/text-datasets/aclImdb.zip
原始 IMDB 电影评论数据集(不是使用 Keras 内置的已经预先分词的 IMDB 数据)

Keras 内置的IMDB 数据
将从https://s3.amazonaws.com/text-datasets/imdb.npz 下载数据到：.keras/datasets/imdb.npz
链接：https://pan.baidu.com/s/1L7rNOHsFsAJSNirWM4ykMw 密码：kjpa

wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
tar zxvf aclImdb_v1.tar.gz

https://github.com/charlesliucn/kaggle-in-python/tree/master/kaggle_competitions/IMDB
labeledTrainData.tsv  testData.tsv  unlabeledTrainData.tsv

2014 年英文维基百科的预计算嵌入
这是一个 822 MB 的压缩文件,文件名是 glove.6B.zip,里面包含 400 000 个单词(或非单词的标记)的 100 维嵌入向量。
http://nlp.stanford.edu/data/glove.6B.zip

glove.6B.zip
http://nlp.stanford.edu/data/glove.6B.zip

gswyhq@gswyhq-PC:~/data/glove.6B$ unzip ../glove.6B.zip 
Archive:  ../glove.6B.zip
  inflating: glove.6B.50d.txt        
  inflating: glove.6B.100d.txt       
  inflating: glove.6B.200d.txt       
  inflating: glove.6B.300d.txt  

花卉图像数据集下载
http://download.tensorflow.org/example_images/flower_photos.tgz
5种花卉类型，接近4000张图像，分为训练集与测试集。

atec_nlp_sim_train.csv
相似度匹配语料
数据来源： https://github.com/ziweipolaris/atec2018-nlp.git

同义句语料：
中文文本语义相似度（Chinese Semantic Text Similarity）语料库建设
https://github.com/IAdmireu/ChineseSTS.git
simtrain_to05sts_same.txt  simtrain_to05sts.txt

保险问答用户日志.xlsx

zhrs_faq_8_updateqa_question_answer.json
一些保险相关相似句

意图识别数据_all.txt

cased_L-24_H-1024_A-16.zip
https://storage.googleapis.com/xlnet/released_models/cased_L-24_H-1024_A-16.zip
https://github.com/zihangdai/xlnet

人脸检测预训练模型
https://github.com/ChiCheng123/SRN
SRN.pth
https://pan.baidu.com/share/init?surl=ambmu1Bu6Oi7zTcEnigFyg 密码：6fba

chinese_wwm_L-12_H-768_A-12.zip
中文全词覆盖BERT模型
https://github.com/ymcui/Chinese-BERT-wwm
https://mp.weixin.qq.com/s/nlFXfgM5KKZXnPdwd97JYg

task_data.tgz 任务数据
ERNIE_stable.tgz 模型，包含预训练模型参数
https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE

ChnSentiCorp_htl_all.csv
7000 多条酒店评论数据，5000 多条正向评论，2000 多条负向评论
https://github.com/SophonPlus/ChineseNlpCorpus/raw/master/datasets/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv

waimai_10k.csv
某外卖平台收集的用户评价，正向 4000 条，负向 约 8000 条
https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/waimai_10k/waimai_10k.csv

online_shopping_10_cats.zip
10 个类别，共 6 万多条评论数据，正、负向评论各约 3 万条， 包括书籍、平板、手机、水果、洗发水、热水器、蒙牛、衣服、计算机、酒店
https://github.com/SophonPlus/ChineseNlpCorpus/raw/master/datasets/online_shopping_10_cats/online_shopping_10_cats.zip

weibo_senti_100k.zip
10 万多条，带情感标注 新浪微博，正负向评论约各 5 万条
https://pan.baidu.com/s/1DoQbki3YwqkuwQUOj64R_g

simplifyweibo_4_moods.zip
36 万多条，带情感标注 新浪微博，包含 4 种情感，其中喜悦约 20 万条，愤怒、厌恶、低落各约 5 万条
https://pan.baidu.com/s/16c93E5x373nsGozyWevITg

dmsc
movies.csv ratings.zip
 28 部电影，超 70 万 用户，超 200 万条 评分/评论 数据
https://pan.baidu.com/s/1c0yn3TlkzHYTdEBz3T5arA#list/path=%2F

yf_dianping
ratings.zip links.csv restaurants.csv
https://pan.baidu.com/s/1yMNvHLl6QYsGbjT7u51Nfg
24 万家餐馆，54 万用户，440 万条评论/评分数据

yf_amazon
https://pan.baidu.com/s/1SbfpZb5cm-g2LmnYV_af8Q
52 万件商品，1100 多个类目，142 万用户，720 万条评论/评分数据

微博实体识别.
https://github.com/hltcoe/golden-horse

ez_douban
5 万多部电影（3 万多有电影名称，2 万多没有电影名称），2.8 万 用户，280 万条评分数据
https://pan.baidu.com/s/1DkN1LmdSMzm_jCBKhbPbig

ACL2019-DuConv
数据下载地址：
http://ai.baidu.com/broad/download

Knowledge Extraction
知识抽取，标注的三元组数据
https://github.com/chenwanyuan/knowledge_extraction/blob/master/data/corpu.tar.gz

MS-Celeb-1M 名人图片数据集
https://hyper.ai/datasets/5543

CCKS 2018 微众银行智能客服问句匹配大赛
10w对左右的标注训练问句对数据集和1w对左右的验证问句对数据集。
https://biendata.com/competition/CCKS2018_3/datadescribe/
https://github.com/zoulala/CCKS_QA/blob/master/data/task3_train.txt

# 旅行险日志：
zdal_log

语义匹配语料：
new_esim
LCQMC: http://icrc.hitsz.edu.cn/info/1037/1146.htm
sentencesim
chinese_sts_corpus
ChineseSTSCorpus = CCKS + LCQMC + sentencesim + chinese_sts_corpus

Iris Data Set(鸢尾属植物数据集)
iris.data 
http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data

腾讯超过800万个中文单词和短语提供200维矢量表示
https://ai.tencent.com/ailab/nlp/data/Tencent_AILab_ChineseEmbedding.tar.gz

cipslong.v1
搜狗阅读理解比赛	大概80k的数据集

dureader_raw
阅读理解数据，百度dureader	100k的问答集

ChineseSTSListCorpus
dureader的文章title	20k类，大概可以生成 400K个不重复的句子对

preprocessed
百度知道数据；包括一些相似句等；

nietzsche.txt
#尼采的一些作品
https://s3.amazonaws.com/text-datasets/nietzsche.txt

cc.zh.300.vec
wiki，200万，300维的词向量

WordVector_60dimensional
60维维基百科词向量
https://pan.baidu.com/s/1o8f1ELs

词向量
sgns.target.word-character.char1-1.dynwin5.thr10.neg5.dim300.iter5.bz2
https://github.com/Embedding/Chinese-Word-Vectors
https://pan.baidu.com/s/1c9yiosHKNIZwRlLzD_F1ig

120G+训练好的word2vec模型（中文词向量）
$ md5sum pack.zip
fa9af79358c746805fa61662928e019b  pack.zip
$ unzip pack.zip
Archive:  pack.zip
   creating: dong/word2vec/
 extracting: dong/word2vec/news_12g_baidubaike_20g_novel_90g_embedding_64.model
 extracting: dong/word2vec/news_12g_baidubaike_20g_novel_90g_embedding_64.bin
 extracting: dong/word2vec/news12g_bdbk20g_nov90g_dim128.tar.gz
 extracting: dong/word2vec/news_12g_baidubaike_20g_novel_90g_embedding_64.model.syn0.npy
 模型参数：
 window=5
 min_count=5
 size=64
 ps：其它参数见gensim库，执行代码为：Word2Vec(sentence, window=5, min_count=5,size=64, workers=4)
 其它相关：
 分词词典使用了130w+词典。分词代码：jieba.lcut(sentence)，默认使用了HMM识别新词；
 剔除了所有非中文字符；
 最终得到的词典大小为6115353；
 目前只跑了64维的结果，后期更新128维词向量；
 模型格式有两种bin和model；
 下载链接：链接: https://pan.baidu.com/s/1o7MWrnc 密码:wzqv
 https://weibo.com/p/23041816d74e01f0102x77v?luicode=20000061&lfid=4098518198740187&featurecode=newtitle

官方的中文预训练bert权重
chinese_L-12_H-768_A-12.zip
https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip

情感语料：
neg.xls  pos.xls
https://www.cnblogs.com/sumuncle/p/6370686.html
http://spaces.ac.cn/usr/uploads/2015/08/646864264.zip
https://github.com/zhuanxuhit/nd101/tree/master/1.Intro_to_Deep_Learning/3.How_to_Do_Sentiment_Analysis/data

中文语音语料（13388个片段）
thchs30: 清华大学30小时的数据集，可以在http://www.openslr.org/18/下载
data_thchs30.tgz [6.4G]
http://blog.sina.com.cn/s/blog_8af106960102xdev.html

评论数据
sum.xls
https://www.cnblogs.com/sumuncle/p/6370686.html
http://spaces.ac.cn/usr/uploads/2015/09/829078856.zip

CCKS2019关系抽取数据
- origin_data [origin data](https://pan.baidu.com/s/1EGPYAQp90usvpzROdNilPw) d6vp
- preprocess_data [preprocessed data](https://pan.baidu.com/s/1lHD-IO7zI4EwyNfi_Fe1HQ) 4f93

维基百科`598454`个词 50维的词向量模型，包括原始的训练数据
zhwiki.zip

中文Wiki语料获取
/home/gswyhq/Downloads/zhwiki-latest-pages-articles.xml.bz2
~$ bunzip2 zhwiki-latest-pages-articles.xml.bz2
~$ ls
zhwiki-latest-pages-articles.xml
wiki中文数据的下载地址是：https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2

短文本语义匹配数据集
QA_corpus
https://github.com/terrifyzhao/text_matching/blob/master/input/train.csv

医疗知识图谱NLP项目，实体规模4.4万，实体关系规模30万,实体类型包括：诊断检查项目,医疗科目，疾病，药品，食物，在售药品，疾病症状
https://github.com/liuhuanyong/QASystemOnMedicalKG.git

glyph 文本分类数据：
来源：https://github.com/zhangxiangxiao/glyph
数据集	类	训练	测试
大众点评网(Dianping)	2	2000000	500000
JD全部(JD full)	5	3000000	250000
JD二类(JD binary)	2	4000000	360000
凤凰网(lfeng)	5	800000	50000
中新网(Chinanews)	7	1,400,000	112000

dataSets.zip
中英文语义匹配数据集
中文数据为atec2018数据
https://drive.google.com/file/d/15FTuNdF5YswRMo3USL_lT_h1hI3ylrAc/view?usp=sharing

英文单词音频数据集
speech_commands_v0.01.tar.gz
https://pan.baidu.com/s/1Au85kI_oeDjode2hWumUvQ
speech_commands_v0.02.tar.gz
http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz


multinli_1.0.zip
https://www.nyu.edu/projects/bowman/multinli/multinli_1.0.zip

snli_1.0.zip
https://nlp.stanford.edu/projects/snli/snli_1.0.zip
/home/gswyhq/github_projects/DIIN-in-Keras/data/snli_1.0

Glove预训练词嵌入
该预训练词嵌入根据斯坦福大学提出的Glove模型进行训练，主要包括如下四个文件：
1） glove.6B：Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download)
2） glove.42B.300d：Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download)
3）glove.840B.300d：Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download)
4）glove.twitter.27B：Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download)
glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
/home/gswyhq/github_projects/DIIN-in-Keras/data/glove.840B.300d.txt

图像数据集
http://www.nurs.or.jp/~nagadomi/animeface-character-dataset/data/animeface-character-dataset.zip
https://www.crcv.ucf.edu/data/Selfie/Selfie-dataset.tar.gz

汉字、词语解释、成语、歇后语
https://github.com/pwxcoo/chinese-xinhua

OPPO手机搜索排序query-title语义匹配数据集。
链接:https://pan.baidu.com/s/1Hg2Hubsn3GEuu4gubbHCzw 提取码:7p3n

MSRA微软亚洲研究院数据集。
5 万多条中文命名实体识别标注数据（包括地点、机构、人物）
https://github.com/InsaneLife/ChineseNLPCorpus/tree/master/NER/MSRA
https://github.com/Determined22/zh-NER-TF/blob/master/data_path/train_data

中文完形填空数据集
https://github.com/ymcui/Chinese-RC-Dataset

英汉互译语料
cmn-eng.zip
http://www.manythings.org/anki/cmn-eng.zip
英法互译语料
http://www.manythings.org/anki/fra-eng.zip

/home/gswyhq/data/ERNIE_1.0/ERNIE_stable-1.0.1.tar.gz
ERNIE 1.0 中文 Base 模型	包含预训练模型参数、词典 vocab.txt、模型配置 ernie_config.json
https://baidu-nlp.bj.bcebos.com/ERNIE_stable-1.0.1.tar.gz

ERNIE_1.0中文数据
/home/gswyhq/data/ERNIE_1.0/task_data_zh.tgz
https://ernie.bj.bcebos.com/task_data_zh.tgz

XNLI 15种语言的翻译语料
包含10,000个句子的15种语言平行语料库
XNLI-15way.zip
https://dl.fbaipublicfiles.com/XNLI/XNLI-15way.zip


跨语言自然语言推理（XNLI）语料库是MultiNLI语料库的5,000个测试和2,500个开发对的众包集合。这对配有文字蕴涵注释，并翻译成14种语言：法语，西班牙语，德语，希腊语，保加利亚语，俄语，土耳其语，阿拉伯语，越南语，泰语，中文，印地语，斯瓦希里语和乌尔都语。
XNLI-1.0.zip
https://dl.fbaipublicfiles.com/XNLI/XNLI-1.0.zip

瑞金医院MMC人工智能辅助构建知识图谱大赛
ner数据：data_source.zip
链接：https://pan.baidu.com/s/1jKUpBdKh_prpZ1Seb3R3cQ
提取码：7h1l

kvret_dataset_public.zip
http://nlp.stanford.edu/projects/kvret/kvret_dataset_public.zip

### 中文文本数据集

- [Chinese Treebank](https://catalog.ldc.upenn.edu/LDC2013T21)：来自中国新闻专线，政府文件，杂志文章和各种广播新闻的大约150万字的注释和解析文本
- [Mandarin Chinese News Text](https://github.com/Lab41/sunny-side-up/wiki/Chinese-Datasets)：人民日报，新华社，中国国际广播电台的2.5亿汉字新闻。
- [腾讯AI实验室嵌入中文单词和短语](https://ai.tencent.com/ailab/nlp/embedding.html)语料库：该语料库为超过800万个中文单词和短语提供200维矢量表示，即嵌入，这些单词和短语是在大规模，高质量数据上预先培训的。
- [大规模中文短文摘要数据集](https://arxiv.org/abs/1506.05865)：该语料库由超过200万个真正的中文短文组成，每个文本的作者给出简短的摘要。

### 中文OCR和手写数据集

- [汉字](http://www.iapr-tc11.org/mediawiki/index.php?title=Harbin_Institute_of_Technology_Opening_Recognition_Corpus_for_Chinese_Characters_(HIT-OR3C))：包含909,818张图像的手写汉字数据集，对应于约10篇新闻文章。
- [汉字生成器](https://www.kaggle.com/dylanli/chinesecharacter)：可用于中文文本OCR的中文字体数据集。
- [中文文本](https://ctwdataset.github.io/)：中文文本数据集，大约有一百万个汉字由专家在超过30,000个街景图像中注释。对于数据集中的每个字符，注释包括其基础字符，边界框和6个属性。属性表明它是否具有复杂的背景，是否有凸起，是手写还是打印等。

### 中文翻译和并行文本数据集

- [中英文电子邮件](http://catalog.elra.info/en-us/repository/browse/ELRA-W0113/)：包含15,000个中文字符（相当于10,000个单词）和电子邮件，以及英文参考译文。
- [OntoNotes](https://catalog.ldc.upenn.edu/ldc2013t19)：[带有](https://catalog.ldc.upenn.edu/ldc2013t19)各种文本类型的注释语料库 - 新闻，会话电话语音，网络日志，usenet新闻组，广播，脱口秀 - 中文，英文和阿拉伯文。
- [新加坡国立大学语料库](http://wing.comp.nus.edu.sg:8080/SMSCorpus/history.jsp)：这个语料库是为社交媒体文本规范化和翻译而创建的。它是通过从新加坡国立大学英语短信语料库中随机选择2,000条消息然后翻译成正式中文而构建的。
- [中法文本](https://catalog.ldc.upenn.edu/LDC2018T17)：中文广播新闻中约30,000个汉字子集的法语翻译。
- [GALE第1阶段中文博客平行文本](https://catalog.ldc.upenn.edu/LDC2008T06)：277个中文博客文章翻译成英文。

### 中国情绪分析数据集

- [Ren-CECps](http://a1-www.is.tokushima-u.ac.jp/member/ren/Ren-CECps1.0/Ren-CECps1.0.html)：1,500篇博文（11k段，35k句），文档段落和句子级别带有情感和情感注释。
- [微博PCU](https://archive.ics.uci.edu/ml/datasets/microblogPCU#)：来自西安交通大学的研究人员，这个数据集有来自新浪微博的50,000个帖子，包括用户元数据，包括跟随者信息。

样例三元组数据：
https://github.com/percent4/Neo4j_movie_demo

LIC2019 信息抽取 实体及关系抽取
https://github.com/melancholicwang/lic2019-information-extraction-baseline

Toxic Comment Classification Challenge
https://raw.githubusercontent.com/gangqing/Toxic-Comment-Classification-Challenge/master/train.csv

AISHELL-1
http://www.openslr.org/resources/33/data_aishell.tgz
AISHELL-ASR0009录音文本涉及智能家居、无人驾驶、工业生产等11个领域。录制过程在安静室内环境中， 同时使用3种不同设备： 高保真麦克风（44.1kHz，16-bit）；Android系统手机（16kHz，16-bit）；iOS系统手机（16kHz，16-bit）。高保真麦克风录制的音频降采样为16kHz，用于制作AISHELL-ASR0009-OS1。400名来自中国不同口音区域的发言人参与录制。经过专业语音校对人员转写标注，并通过严格质量检验，此数据库文本正确率在95%以上。分为训练集、开发集、测试集。（支持学术研究，未经允许禁止商用。）

百科 QA问答数据集
baike_qa_train.json
baike_qa_valid.json

中文八卦闲聊问答语料
https://github.com/zake7749/Gossiping-Chinese-Corpus

一千多个选择题及其答案：
https://github.com/lindianyin/yol/blob/master/setting/quiz/quiz_issue_question.txt
https://github.com/prime51/Brainstorm/blob/master/Brainstorm_GamePage/database/QuestionBank.csv
https://github.com/pagoda-animation/QAhero/blob/master/assets/questions/questions.json

一些文学作品数据，txt格式：
https://github.com/BlankRain/ebooks
https://github.com/CaptionwWaterfall/nihong-novel
https://github.com/HeywoodKing/infinite/tree/master/%E6%94%B6%E5%BD%95

汉语古典文本资料库
本数据库有13000种文本，10万卷，近13亿字，3.14 GB。可以作为对比的是，《四库全书》共收书3503种，79337卷，近230万页，约8亿字。据《中国古籍总目》，全球现存汉语古籍的总量为177107种。
https://github.com/mahavivo/scripta-sinica

DeepFashion 服装数据集
https://www.aiuai.cn/aifarm112.html

维基百科json版(wiki2019zh)
104万个词条(1,043,224条; 原始文件大小1.6G，压缩文件519M；数据更新时间：2019.2.7)
https://pan.baidu.com/s/1uPMlIY3vhusdnhAge318TA

新闻语料json版(news2016zh)
250万篇新闻( 原始数据9G，压缩文件3.6G；新闻内容跨度：2014-2016年)
包含了250万篇新闻。新闻来源涵盖了6.3万个媒体，含标题、关键词、描述、正文。
https://drive.google.com/file/d/1TMKu1FpTr6kcjWXWlQHX7YJsMfhhcVKp/view

百科类问答json版(baike2018qa)
150万个问答( 原始数据1G多，压缩文件663M；数据更新时间：2018年)
baike_qa2019.zip
~$ unzip baike_qa2019.zip
Archive:  baike_qa2019.zip
  inflating: baike_qa_train.json
  inflating: baike_qa_valid.json
https://pan.baidu.com/share/init?surl=2TCEwC_Q3He65HtPKN17cA
fu45
含有150万个预先过滤过的、高质量问题和答案，每个问题属于一个类别。总共有492个类别，其中频率达到或超过10次的类别有434个。

社区问答json版(webtext2019zh) ：大规模高质量数据集
410万个问答( 过滤后数据3.7G，压缩文件1.7G；数据跨度：2015-2016年)
webtext2019zh.zip
https://drive.google.com/file/d/1u2yW_XohbYL2YAK6Bzc5XrngHstQTf0v/view
含有410万个预先过滤过的、高质量问题和回复。每个问题属于一个【话题】，总共有2.8万个各式话题，话题包罗万象。
从1400万个原始问答中，筛选出至少获得3个点赞以上的的答案，代表了回复的内容比较不错或有趣，从而获得高质量的数据集。
除了对每个问题对应一个话题、问题的描述、一个或多个回复外，每个回复还带有点赞数、回复ID、回复者的标签。

翻译语料(translation2019zh)
translation2019zh.zip (596M) 
520万个中英文平行语料( 原始数据1.1G，压缩文件596M)
https://drive.google.com/file/d/1EX8eE5YWBxCaohBO8Fh4e2j3b9C2bTVQ/view
中英文平行语料520万对。每一个对，包含一个英文和对应的中文。中文或英文，多数情况是一句带标点符号的完整的话。
对于一个平行的中英文对，中文平均有36个字，英文平均有19个单词(单词如“she”)

500条中文情感分析语料：
https://github.com/Zephery/weiboanalysis/blob/master/train/train.txt
其中1表示积极，2表示消极，3表示客观

NLPCC 2018 开放领域问答
http://tcci.ccf.org.cn/conference/2018/taskdata.php
训练数据：
https://pan.baidu.com/s/1dGtGmBZ
密码：f77i
测试数据：http://tcci.ccf.org.cn/conference/2018/dldoc/tasktestdata07.zip

NLPCC 2017 开放领域问答
http://tcci.ccf.org.cn/conference/2017/taskdata.php
训练数据：http://pan.baidu.com/s/1dEYcQXz
测试数据：http://tcci.ccf.org.cn/conference/2017/dldoc/tasktestdata05.zip

NLPCC 2016 开放领域问答
http://tcci.ccf.org.cn/conference/2016/pages/page05_evadata.html
训练数据：https://github.com/WenRichard/KBQA-BERT/tree/master/Data/NLPCC2016KBQA
https://github.com/huangxiangzhou/NLPCC2016KBQA.git
测试数据：http://tcci.ccf.org.cn/conference/2016/dldoc/evatestdata2-kbqa.testing-data
测试数据：http://tcci.ccf.org.cn/conference/2016/dldoc/evatestdata2-dbqa.testing-data

中文NL2SQL
/home/gswyhq/github_projects/nl2sql/data/train
https://github.com/eguilg/nl2sql.git

中文预训练模型 XLNet-mid, Chinese, TensorFlow 
XLNet-mid：24-layer, 768-hidden, 12-heads, 209M parameters
chinese_xlnet_mid_L-24_H-768_A-12.zip (741M) 
https://drive.google.com/open?id=1342uBc7ZmQwV6Hm6eUIN_OnBSz1LcvfA
https://github.com/ymcui/Chinese-PreTrained-XLNet

人脸识别数据 widerface
wider_face_add_lm_10_10.zip 3.06G
（1）过滤掉10px*10px 小人脸后的干净widerface数据压缩包 ：https://pan.baidu.com/s/1m600pp-AsNot6XgIiqDlOw 提取码：x5gt
wider_face.zip 3.42G
（2）未过滤小人脸的完整widerface数据压缩包 ：https://pan.baidu.com/s/1ijvZFSb3l7C63Nbz7i6IuQ 提取码：8748

开源生物识别数据
features.tar.gz 2.4G
Google Audioset：扩展了 632 个音频分类样本，并从 YouTube 视频中提取了 2，084，320 个人类标记的 10 秒声音片段
地址：https://research.google.com/audioset/
https://research.google.com/audioset/download.html
https://github.com/audioset/ontology.git

chineseGLUEdatasets
https://github.com/chineseGLUE/chineseGLUE
https://storage.googleapis.com/chineseglue/chineseGLUEdatasets.v0.0.1.zip

英文命名实体识别数据：
/home/gswyhq/.keras/datasets/conll2000.zip
https://master.dl.sourceforge.net/project/nltk/OldFiles/conll2000.zip

中文机器阅读理解评测（CMRC 2018）]所使用的数据
原始数据(Original Data)
(https://worksheets.codalab.org/bundles/0x0e6dced40a0d40d980da88882aa8c13a) | cmrc2018_evaluate.py | 4.1k 
(https://worksheets.codalab.org/bundles/0xcd4c755829064426896ef942a249aced) | cmrc2018_trial.json  | 1.0m                  
(https://worksheets.codalab.org/bundles/0x296baa11dfbc4ab08cdeb5b4adf182e2) | cmrc2018_train.json  | 9.0m                
(https://worksheets.codalab.org/bundles/0xb70e5e281fcd437d9aa8f1c4da107ae4) | cmrc2018_dev.json    | 3.4m
SQuAD样式的数据(SQuAD-style Data)
(https://worksheets.codalab.org/bundles/0x1ecd59f275a64bbd8cd79868ed059204) | cmrc2018_evaluate.py | 4.2k
(https://worksheets.codalab.org/bundles/0x182c2e71fac94fc2a45cc1a3376879f7) | cmrc2018_trial.json | 781k 
(https://worksheets.codalab.org/bundles/0x15022f0c4d3944a599ab27256686b9ac) | cmrc2018_train.json | 7.1m
(https://worksheets.codalab.org/bundles/0x72252619f67b4346a85e122049c3eabd) | cmrc2018_dev.json  | 3.1m

pytorch版本，预训练的bert模型
Tokenizer需要的是词表: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt
模型：https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz
tar -zxvf bert-base-chinese.tar.gz 
./pytorch_model.bin
./bert_config.json
$md5sum bert-base-chinese.tar.gz 
22ab8d5c0f45ab0705cfff2cebcc53b4  bert-base-chinese.tar.gz
nohup wget -c -t 0 https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz -O bert-base-chinese.tar.gz > bert-base-chinese.log &
nohup wget -c -t 0 https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt -O bert-base-chinese-vocab.txt > bert-base-chinese-vocab.log &
nohup wget -c -t 0 https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin > bert-base-chinese-pytorch_model.log &
最后下载的`bert-base-chinese-pytorch_model.bin`文件，实际上是与`bert-base-chinese.tar.gz`解压出来的文件`pytorch_model.bin`是同一个文件；

繁体中文阅读理解语料：DRCD
https://github.com/DRCKnowledgeTeam/DRCD

食品，餐饮情感分析二分类数据：
https://github.com/YIMIxxxxx/NLP-bert-try

法研杯CAIL2019阅读理解数据
CAIL2019-RC-big.zip
数据集：https://pan.baidu.com/s/1p4NJDhboKSsbFQOwRgDszg 提取码：8w0y

15亿参数 GPT2 中文预训练模型( 15G 语料，训练 10w 步 )
model.ckpt-100000.data-00000-of-00001 (5.2G) 
https://drive.google.com/open?id=1n_5-tgPpQ1gqbyLPbP1PwiFi2eo7SWw_

pytorch: https://github.com/qywu/Chinese-GPT
encoder.pth
https://drive.google.com/uc?id=1Mr2-x_qT2hgyo0RalPjc09NmyNi6a_gs&export=download
model_state_epoch_62.th
https://drive.google.com/uc?id=1W6n7Kv6kvHthUX18DhdGSzBYkyzDvxYh&export=download

