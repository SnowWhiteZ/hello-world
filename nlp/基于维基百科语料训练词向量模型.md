
维基百科简体中文语料的获取

第一步，下载中文的 Wiki Dump
　　链接是：http://download.wikipedia.com/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2。
这个压缩包里面存的是标题、正文部分，如果需要其他数据，如页面跳转、历史编辑记录等，可以到目录下找别的下载链接。
　　

第二步，使用 Wikipedia Extractor 抽取正文文本
　　Wikipedia Extractor(https://github.com/attardi/wikiextractor) 是意大利人用 Python 写的一个维基百科抽取器，使用非常方便。下载之后直接使用这条命令即可完成抽取，运行了大约半小时的时间。
　　gswyhq@gswyhq-PC:~/data/zhwiki$ time python3 /home/gswyhq/github_projects/wikiextractor/WikiExtractor.py /home/gswyhq/data/zhwiki-latest-pages-articles.xml.bz2 -b 1000M -o extracted > /dev/null 2>&1 > output.txt
　　参数 -b 1000M 表示以 1000M 为单位切分文件，默认是 500K。由于最后生成的正文文本有 1300M，可以再把参数设置的大一些可以保证最后的抽取结果全部存在一个文件里。
    extracted: 是抽取到的文本保存目录；
　　.
├── extracted
│   └── AA
│       ├── wiki_00
│       └── wiki_01
├── output.txt

追加 wiki_01 到 wiki_00： cat wiki_01 >> wiki_00

第三步，繁简转换
gswyhq@gswyhq-PC:~/data/zhwiki/extracted/AA$ docker run --rm -it -v $PWD:/tmp 1docker/opencc opencc -c tw2s -i /tmp/wiki_00 -o /tmp/wiki_chs
wiki_00 这个文件是此前使用 Wikipedia Extractor 得到的。

gswyhq@gswyhq-PC:~/data/zhwiki/extracted/AA$ ls
wiki_00  wiki_01  wiki_chs
gswyhq@gswyhq-PC:~/data/zhwiki/extracted/AA$ du -h wiki_chs
1.3G	wiki_chs

第四步： 分词
gswyhq@gswyhq-PC:~/data/zhwiki$ time python3 jieba_cut_word.py extracted/AA/wiki_chs extracted/AA/wiki_chs.seg.txt
```python
#!/usr/bin/python3

import sys
import os
import jieba
import jieba.analyse
import codecs
 
def prepareData(sourceFile, targetFile):
    f =codecs.open(sourceFile, 'r', encoding='utf-8')
    target = codecs.open(targetFile, 'w', encoding='utf-8')
    print( 'open source file: '+ sourceFile )
    print( 'open target file: '+ targetFile )
     
    lineNum = 0
    for eachline in f:
        lineNum += 1
        if lineNum % 1000 == 0:
            print('---processing ', sourceFile, lineNum,' article---')
        seg_list = jieba.cut(eachline, cut_all=False)
        line_seg = ' '.join(seg_list)
        target.write(line_seg+ '\n')
    print('---Well Done!!!---' * 4)
    f.close()
    target.close()
     
# sourceFile = 'wiki.zh.simp.txt'
# targetFile = 'wiki.zh.simp.seg.txt'
sourceFile = sys.argv[1]
targetFile = sys.argv[2]
prepareData(sourceFile, targetFile)

```
第五步： 训练
gswyhq@gswyhq-PC:~/data/zhwiki$ time python3 train_word2vec.py extracted/AA/wiki_chs.seg.txt extracted/AA/wiki_chs.seg.model extracted/AA/wiki_chs.seg.vector

```python
#!/usr/bin/python3

import os
import sys
import logging
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence
import multiprocessing
 
program = os.path.basename(sys.argv[0])
logger = logging.getLogger(program)
logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s',level=logging.INFO)
logger.info("running %s" % ' '.join(sys.argv))
 
# inp为输入语料, outp1 为输出模型, outp2为原始c版本word2vec的vector格式的模型
# inp = 'wiki.zh.simp.seg.txt'
# outp1 = 'wiki.zh.text.model'
# outp2 = 'wiki.zh.text.vector'

inp = sys.argv[1]
outp1 = sys.argv[2]
outp2 = sys.argv[3]

#训练skip-gram 模型
model = Word2Vec( LineSentence(inp), size=400, window=5, min_count=5, workers=multiprocessing.cpu_count() )
model.save(outp1)
model.wv.save_word2vec_format(outp2, binary=False)

# 保存模型，以便重用
#model.save(u"computer.model")
# 对应的加载方式
# model_2 = word2vec.Word2Vec.load("text8.model")

# 以一种C语言可以解析的形式存储词向量
#model.save_word2vec_format(u"computer.model.bin", binary=True)
# 对应的加载方式
# from gensim.models.keyedvectors import KeyedVectors
# model3 = KeyedVectors.load_word2vec_format("computer.model.bin", binary=True)

```
训练完(约训练了2小时)，会得到以下四个文件
root@bbd18f140f31:/zhwiki# ls
wiki_chs.seg.model  wiki_chs.seg.model.trainables.syn1neg.npy  wiki_chs.seg.model.wv.vectors.npy wiki_chs.seg.vector

七万多个单词，400维的词向量：
root@bbd18f140f31:/zhwiki# head -n 1 wiki_chs.seg.vector 
711155 400

加载词向量文件
```shell
>>> from gensim.models import KeyedVectors
>>> wv_from_text = KeyedVectors.load_word2vec_format("/zhwiki/wiki_chs.seg.vector", binary=False)
# 计算某个词的相关词列表
>>> wv_from_text.most_similar("刘德华", topn=5)
[('张曼玉', 0.857553243637085), ('郭富城', 0.8571487665176392), ('谢霆锋', 0.8440361022949219), ('周润发', 0.8349112868309021), ('梁朝伟', 0.8346537351608276)]
# 计算两个词的相似度/相关程度
>>> wv_from_text.similarity("火车", "地铁")
0.37193766
# 共有 711155 个词向量
>>> len(wv_from_text.index2word)
711155
# 每个词向量为400维
>>> wv_from_text.word_vec("你好").shape
(400,)
```