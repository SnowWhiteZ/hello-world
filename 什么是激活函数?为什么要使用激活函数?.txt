

什么是激活函数?为什么要使用激活函数?

如果没有 relu 等激活函数(也叫非线性), Dense 层将只包含两个线性运算——点积和加法:
output = dot(W, input) + b
这样 Dense 层就只能学习输入数据的线性变换(仿射变换):该层的假设空间是从输入数据到 16 位空间所有可能的线性变换集合。
这种假设空间非常有限,无法利用多个表示层的优势,因为多个线性层堆叠实现的仍是线性运算,添加层数并不会扩展假设空间。
为了得到更丰富的假设空间,从而充分利用多层表示的优势,你需要添加非线性或激活函数。
relu 是深度学习中最常用的激活函数,但还有许多其他函数可选,它们都有类似的奇怪名称,比如 prelu 、 elu 等。

sigmoid将一个real value映射到（0,1）的区间，用来做二分类。
而 softmax 把一个 k 维的real value向量（a1,a2,a3,a4….）映射成一个（b1,b2,b3,b4….），其中 bi 是一个 0～1 的常数，输出神经元之和为 1.0，所以相当于概率值，然后可以根据 bi 的概率大小来进行多分类的任务。
二分类问题时 sigmoid 和 softmax 是一样的，求的都是 cross entropy loss（交叉熵损失），而 softmax 可以用于多分类问题
sigmoid激活函数的定义域能够取任何范围的实数，而返回的输出值在0到1的范围内。sigmoid函数也被称为S型函数
Softmax函数计算事件超过’n’个不同事件的概率分布。一般来说，这个函数将会计算每个目标类别在所有可能的目标类中的概率。计算出的概率将有助于确定给定输入的目标类别。
使用Softmax的主要优点是输出概率的范围，范围为0到1，所有概率的和将等于1。如果将softmax函数用于多分类模型，它会返回每个类别的概率，并且目标类别的概率值会很大。
在实际应用中，一般将softmax用于多类分类的使用之中，而将sigmoid用于多标签分类之中
sigmoid一般不用来做多类分类，而是用来做二分类，它是将一个标量数字转换到[0,1]之间，如果大于一个概率阈值(一般是0.5)，
则认为属于某个类别，否则不属于某个类别。这一属性使得其适合应用于多标签分类之中，
在多标签分类中，大多使用binary_crossentropy损失函数。
sigmoid应该会将logits中每个数字都变成[0,1]之间的概率值，假设结果为[0.01, 0.05, 0.4, 0.6, 0.3, 0.1, 0.5, 0.4, 0.06, 0.8],
然后设置一个概率阈值，比如0.3，如果概率值大于0.3，则判定类别符合，那么该输入样本则会被判定为类别3、类别4、类别5、类别7及类别8。
即一个样本具有多个标签。


