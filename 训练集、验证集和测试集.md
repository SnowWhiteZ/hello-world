
训练集、验证集和测试集

评估模型的重点是将数据划分为三个集合:训练集、验证集和测试集。在训练数据上训练模型,在验证数据上评估模型。
一旦找到了最佳参数,就在测试数据上最后测试一次。

你可能会问,为什么不是两个集合:一个训练集和一个测试集?
在训练集上训练模型,然后在测试集上评估模型。
这样简单得多!原因在于开发模型时总是需要调节模型配置,比如选择层数或每层大小[这叫作模型的超参数(hyperparameter),以便与模型参数(即权重)区分开]。
这个调节过程需要使用模型在验证数据上的性能作为反馈信号。
这个调节过程本质上就是一种学习:在某个参数空间中寻找良好的模型配置。
因此,如果基于模型在验证集上的性能来调节模型配置,会很快导致模型在验证集上过拟合,即使你并没有在验证集上直接训练模型也会如此。
造成这一现象的关键在于信息泄露(information leak)。
每次基于模型在验证集上的性能来调节模型超参数,都会有一些关于验证数据的信息泄露到模型中。
如果对每个参数只调节一次,那么泄露的信息很少,验证集仍然可以可靠地评估模型。
但如果你多次重复这一过程(运行一次实验,在验证集上评估,然后据此修改模型),那么将会有越来越多的关于验证集的信息泄露到模型中。
最后,你得到的模型在验证集上的性能非常好(人为造成的),因为这正是你优化的目的。
你关心的是模型在全新数据上的性能,而不是在验证数据上的性能,因此你需要使用一个完全不同的、前所未见的数据集来评估模型,它就是测试集。
你的模型一定不能读取与测试集有关的任何信息,既使间接读取也不行。如果基于测试集性能来调节模型,那么对泛化能力的衡量是不准确的。
将数据划分为训练集、验证集和测试集可能看起来很简单,但如果可用数据很少,还有几种高级方法可以派上用场。
我们先来介绍三种经典的评估方法:简单的留出验证、K 折验证,以及带有打乱数据的重复 K 折验证。

1. 简单的留出验证
留出一定比例的数据作为测试集。在剩余的数据上训练模型,然后在测试集上评估模型。
如前所述,为了防止信息泄露,你不能基于测试集来调节模型,所以还应该保留一个验证集。

```python
# 留出验证
num_validation_samples = 10000

# 通常需要打乱数据
np.random.shuffle(data)

validation_data = data[:num_validation_samples]

# 定义验证集
data = data[num_validation_samples:]

# 定义训练集
training_data = data[:]

# 在训练数据上训练模型,并在验证数据上评估模型
model = get_model()
model.train(training_data)
validation_score = model.evaluate(validation_data)


# 现在你可以调节模型、重新训练、评估,然后再次调节......
# 一旦调节好超参数,通常就在所有非测试数据上从头开始训练最终模型
model = get_model()
model.train(np.concatenate([training_data,
validation_data]))
test_score = model.evaluate(test_data)
```

这是最简单的评估方法,但有一个缺点:
如果可用的数据很少,那么可能验证集和测试集包含的样本就太少,从而无法在统计学上代表数据。
这个问题很容易发现:如果在划分数据前进行不同的随机打乱,最终得到的模型性能差别很大,那么就存在这个问题。
接下来会介绍 K 折验证与重复的 K 折验证,它们是解决这一问题的两种方法。

2. K 折验证
K 折验证(K-fold validation)将数据划分为大小相同的 K 个分区。
对于每个分区 i ,在剩余的 K - 1 个分区上训练模型,然后在分区 i 上评估模型。
最终分数等于 K 个分数的平均值。对于不同的训练集 - 测试集划分,如果模型性能的变化很大,那么这种方法很有用。
与留出验证一样,这种方法也需要独立的验证集进行模型校正。

```python
# K 折交叉验证
k = 4
num_validation_samples = len(data) // k

np.random.shuffle(data)
validation_scores = []
for fold in range(k):
    validation_data = data[num_validation_samples * fold:
    # 选择验证数据分区
    num_validation_samples * (fold + 1)]
    training_data = data[:num_validation_samples * fold] +
    data[num_validation_samples * (fold + 1):]
    # 使用剩余数据作为训练数据。注意,+ 运算符是列表合并,不是求和

    # 创建一个全新的模型
    model = get_model()
    model.train(training_data)
    validation_score = model.evaluate(validation_data)

    validation_scores.append(validation_score)
    # 实例(未训练)

# 最终验证分数:K 折验证分数的平均值
validation_score = np.average(validation_scores)

# 在所有非测试数据上训练最终模型
model = get_model()
model.train(data)
test_score = model.evaluate(test_data)


```

如果可用的数据相对较少,而你又需要尽可能精确地评估模型,那么可以选择带有打乱数据的重复 K 折验证(iterated K-fold validation with shuffling)。
我发现这种方法在 Kaggle 竞赛中特别有用。具体做法是多次使用 K 折验证,在每次将数据划分为 K 个分区之前都先将数据打乱。
最终分数是每次 K 折验证分数的平均值。注意,这种方法一共要训练和评估 P×K 个模型(P是重复次数),计算代价很大。


