
1,安装
gswyhq@gswyhq-PC:~$ sudo pip3 install pyltp

2,下载模型
模型文件下载地址： http://pan.baidu.com/share/link?shareid=1988562907&uk=2738088569
或：http://ltp.ai/download.html

gswyhq@gswyhq-PC:~/Downloads$ unzip ltp_data_v3.4.0.zip -d ~/data/
Archive:  ltp_data_v3.4.0.zip
   creating: /home/gswyhq/data/ltp_data_v3.4.0/

4 基本组件使用
4.1 分句
from pyltp import SentenceSplitter
sents = SentenceSplitter.split('元芳你怎么看？我就趴窗口上看呗！')  # 分句
print('\n'.join(sents))

输出：
元芳你怎么看？
我就趴窗口上看呗！

4.2 分词
import os
from pyltp import Segmentor
LTP_DATA_DIR='/home/gswyhq/data/ltp_data_v3.4.0'
cws_model_path=os.path.join(LTP_DATA_DIR,'cws.model')
segmentor=Segmentor()
segmentor.load(cws_model_path)
words=segmentor.segment('熊高雄你吃饭了吗')
print(type(words))
print('\t'.join(words))
segmentor.release()

输出
熊高雄 你   吃饭  了   吗

4.3 使用自定义词典
lexicon.txt文件如下：

保险产品
安行万里
亚硝酸盐






import os
cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`

from pyltp import Segmentor
segmentor = Segmentor()  # 初始化实例
segmentor.load_with_lexicon(cws_model_path, '/home/gswyhq/Downloads/lexicon.txt') # 加载模型，第二个参数是您的外部词典文件路径
words = segmentor.segment('亚硝酸盐是一种化学物质,保险产品与安行万里分别是什么保险产品？')
print('\t'.join(words))
segmentor.release()

输出
[INFO] 2018-08-16 19:18:03 loaded 2 lexicon entries
亚硝酸盐	是	一	种	化学	物质	,	保险产品	与	安行万里	分别	是	什么	保险	产品	？

4.4 词性标注
import os

# ltp模型目录的路径
pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`

from pyltp import Postagger
postagger = Postagger() # 初始化实例
postagger.load(pos_model_path)  # 加载模型

words = ['元芳', '你', '怎么', '看']  # 分词结果
postags = postagger.postag(words)  # 词性标注

print('\t'.join(postags))
postagger.release()  # 释放模型


输出如下
nh      r       r       v

4.5 命名实体识别
import os
#LTP_DATA_DIR # ltp模型目录的路径
ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')  # 命名实体识别模型路径，模型名称为`pos.model`

from pyltp import NamedEntityRecognizer
recognizer = NamedEntityRecognizer() # 初始化实例
recognizer.load(ner_model_path)  # 加载模型

words = ['元芳', '你', '怎么', '看']
postags = ['nh', 'r', 'r', 'v']
netags = recognizer.recognize(words, postags)  # 命名实体识别

print('\t'.join(netags))
recognizer.release()  # 释放模型

输出
S-Nh    O   O   O

4.6 依存句法分析
import os
#LTP_DATA_DIR='D:\Data\ltp_data_v3.4.0'  # ltp模型目录的路径
par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  # 依存句法分析模型路径，模型名称为`parser.model`

from pyltp import Parser
parser = Parser() # 初始化实例
parser.load(par_model_path)  # 加载模型

words = ['元芳', '你', '怎么', '看']
postags = ['nh', 'r', 'r', 'v']
arcs = parser.parse(words, postags)  # 句法分析

print("\t".join("%d:%s" % (arc.head, arc.relation) for arc in arcs))
parser.release()  # 释放模型

输出为:
4:SBV   4:SBV   4:ADV   0:HED

标注集请参考 依存句法关系 。
4.7 语义角色标注
import os
#LTP_DATA_DIR='D:\Data\ltp_data_v3.4.0'  # ltp模型目录的路径
srl_model_path = os.path.join(LTP_DATA_DIR, 'pisrl.model')  # 语义角色标注模型目录路径，模型目录为`srl`。注意该模型路径是一个目录，而不是一个文件。

from pyltp import SementicRoleLabeller
labeller = SementicRoleLabeller() # 初始化实例
labeller.load(srl_model_path)  # 加载模型

words = ['元芳', '你', '怎么', '看']
postags = ['nh', 'r', 'r', 'v']
# arcs 使用依存句法分析的结果
roles = labeller.label(words, postags, arcs)  # 语义角色标注

# 打印结果
for role in roles:
    print(role.index, "".join(
        ["%s:(%d,%d)" % (arg.name, arg.range.start, arg.range.end) for arg in role.arguments]))
labeller.release()  # 释放模

输出：
3 A0:(1,1)ADV:(2,2)
