
# 类(class)
    在机器学习中,分类问题中的某个类别叫作类(class)。

# 样本(sample)
    数据点叫作样本(sample)。

# 标签(label)
    某个样本对应的类叫作标签(label)。

# 张量(tensor)
    是一个数据容器。它包含的数据几乎总是数值数据,因此它是数字的容器。你可能对矩阵很熟悉,它是二维张量。
    张量是矩阵向任意维度的推广[注意,张量的维度(dimension)通常叫作轴(axis)]。

# 维度(dimension)
    可以表示沿着某个轴上的元素个数(比如 5D 向量),也可以表示张量中轴的个数(比如 5D 张量),这有时会令人感到混乱。
    对于后一种情况,技术上更准确的说法是 5 阶张量(张量的阶数即轴的个数),但 5D 张量这种模糊的写法更常见。

# 轴(axis)

# 样本轴
    通常来说,深度学习中所有数据张量的第一个轴(0 轴,因为索引从 0 开始)都是样本轴(samples axis,有时也叫样本维度)

# 批量
    深度学习模型不会同时处理整个数据集,而是将数据拆分成小批量。
    具体来看,下面是 MNIST 数据集的一个批量,批量大小为 128。
    batch = train_images[:128]
    然后是下一个批量。
    batch = train_images[128:256]
    然后是第 n 个批量。
    batch = train_images[128 * n:128 * (n + 1)]
    对于这种批量张量,第一个轴(0 轴)叫作批量轴(batch axis)或批量维度(batch dimension)。

# 向量数据
    2D 张量,形状为 (samples, features) 。
    对于这种数据集,每个数据点都被编码为一个向量,因此一个数据批量就被编码为 2D 张量(即向量组成的数组),其中第一个轴是样本轴,第二个轴是特征轴。

# 时间序列数据或序列数据
    3D 张量,形状为 (samples, timesteps, features) 。
    当时间(或序列顺序)对于数据很重要时,应该将数据存储在带有时间轴的 3D 张量中。
    每个样本可以被编码为一个向量序列(即 2D 张量),因此一个数据批量就被编码为一个 3D 张量。
    根据惯例,时间轴始终是第 2 个轴(索引为 1 的轴)。

# 图像
    4D 张量,形状为 (samples, height, width, channels) 或 (samples, channels,height, width) 。
    图像通常具有三个维度:高度、宽度和颜色深度。
    虽然灰度图像(比如 MNIST 数字图像)只有一个颜色通道,因此可以保存在 2D 张量中,但按照惯例,图像张量始终都是 3D 张量,灰度图像的彩色通道只有一维。
    因此,如果图像大小为 256×256,那么 128 张灰度图像组成的批量可以保存在一个形状为 (128, 256, 256, 1) 的张量中,
    而 128 张彩色图像组成的批量则可以保存在一个形状为 (128, 256, 256, 3) 的张量中。
    图像张量的形状有两种约定:通道在后(channels-last)的约定(在 TensorFlow 中使用)和通道在前(channels-first)的约定(在 Theano 中使用)。
    Google 的 TensorFlow 机器学习框架将颜色深度轴放在最后: (samples, height, width, color_depth) 。
    与此相反,Theano将图像深度轴放在批量轴之后: (samples, color_depth, height, width) 。
    如果采用 Theano 约定,前面的两个例子将变成 (128, 1, 256, 256) 和 (128, 3, 256, 256) 。
    Keras 框架同时支持这两种格式。

# 视频
    5D 张量,形状为 (samples, frames, height, width, channels) 或 (samples,frames, channels, height, width) 。
    视频数据是现实生活中需要用到 5D 张量的少数数据类型之一。视频可以看作一系列帧,每一帧都是一张彩色图像。
    由于每一帧都可以保存在一个形状为 (height, width, color_depth) 的 3D 张量中,
    因此一系列帧可以保存在一个形状为 (frames, height, width, color_depth) 的 4D 张量中,
    而不同视频组成的批量则可以保存在一个 5D 张量中,其形状为(samples, frames, height, width, color_depth) 。

# 广播(broadcast)
    将一个 2D 张量与一个向量相加。如果将两个形状不同的张量相加,如果没有歧义的话,较小的张量会被广播(broadcast),以匹配较大张量的形状。
    广播包含以下两步。
    (1) 向较小的张量添加轴(叫作广播轴),使其 ndim 与较大的张量相同。
    (2) 将较小的张量沿着新轴重复,使其形状与较大的张量相同。

# 张量点积
    点积运算,也叫张量积(tensor product,不要与逐元素的乘积弄混),是最常见也最有用的张量运算。
    与逐元素的运算不同,它将输入张量的元素合并在一起。
    注意,两个向量之间的点积是一个标量,而且只有元素个数相同的向量之间才能做点积。
    对一个矩阵 x 和一个向量 y 做点积,返回值是一个向量,其中每个元素是 y 和 x的每一行之间的点积。

# 张量变形(tensor reshaping)
    张量变形是指改变张量的行和列,以得到想要的形状。变形后的张量的元素总个数与初始张量相同。
    简单的例子可以帮助我们理解张量变形。
    >>> x = np.array([[0., 1.],
                                    [2., 3.],
                                    [4., 5.]])
    >>> print(x.shape)
    (3, 2)
    >>> x = x.reshape((6, 1))
    >>> x
    array([[ 0.],
    [ 1.],
    [ 2.],
    [ 3.],
    [ 4.],
    [ 5.]])
    >>> x = x.reshape((2, 3))
    >>> x
    array([[ 0., 1., 2.],
                [ 3., 4., 5.]])

# 阶(rank)
    张量轴的个数也叫作阶(rank)。
    例如,3D 张量有 3 个轴,矩阵有 2 个轴。这在 Numpy 等 Python 库中也叫张量的 ndim 。
    >>> x = np.random.random((3, 2))
    >>> x
    array([[0.8957804 , 0.96712402],
           [0.73855227, 0.34115149],
           [0.36667734, 0.16249242]])
    >>> x.ndim
    2

# 形状
    这是一个整数元组,表示张量沿每个轴的维度大小(元素个数)。
    例如,前面矩阵示例的形状为 (3, 5) ,3D 张量示例的形状为 (3, 3, 5) 。
    向量的形状只包含一个元素,比如 (5,) ,而标量的形状为空,即 () 。
    >>> x = np.array(3)
    >>> x.shape
    ()
    >>> x = np.array([2,3])
    >>> x.shape
    (2,)

# 数据类型(在 Python 库中通常叫作 dtype )
    这是张量中所包含数据的类型,例如,张量的类型可以是 float32 、 uint8 、 float64 等。
    在极少数情况下,你可能会遇到字符( char )张量。
    注意,Numpy(以及大多数其他库)中不存在字符串张量,因为张量存储在预先分配的连续内存段中,而字符串的长度是可变的,无法用这种方式存储。
    >>> x = np.array([2.0,3])
    >>> x.dtype
    dtype('float64')

# 标量(scalar, 0D 张量)
    仅包含一个数字的张量叫作标量(scalar,也叫标量张量、零维张量、0D 张量)。
    在 Numpy中,一个 float32 或 float64 的数字就是一个标量张量(或标量数组)。
    你可以用 ndim 属性来查看一个 Numpy 张量的轴的个数。标量张量有 0 个轴( ndim == 0 )。
    张量轴的个数也叫作阶(rank)。下面是一个 Numpy 标量。
    >>> import numpy as np
    >>> x = np.array(12)
    >>> x
    array(12)
    >>> x.ndim
    0

# 向量(vector, 1D 张量)
    数字组成的数组叫作向量(vector)或一维张量(1D 张量)。一维张量只有一个轴。
    下面是一个 Numpy 向量。
    >>> x = np.array([12, 3, 6, 14, 7])
    >>> x
    array([12, 3, 6, 14, 7])
    >>> x.ndim
    1

    这个向量有 5 个元素,所以被称为 5D 向量。不要把 5D 向量和 5D 张量弄混!
    5D 向量只有一个轴,沿着轴有 5 个维度,而 5D 张量有 5 个轴(沿着每个轴可能有任意个维度)。

# 矩阵(matrix, 2D 张量)
    向量组成的数组叫作矩阵(matrix)或二维张量(2D 张量)。
    矩阵有 2 个轴(通常叫作行和列)。你可以将矩阵直观地理解为数字组成的矩形网格。
    下面是一个 Numpy 矩阵。
    >>> x = np.array([[5, 78, 2, 34, 0],
                                    [6, 79, 3, 35, 1],
                                    [7, 80, 4, 36, 2]])
    >>> x.ndim
    2
    第一个轴上的元素叫作行(row),第二个轴上的元素叫作列(column)。
    在上面的例子中,[5, 78, 2, 34, 0] 是 x 的第一行, [5, 6, 7] 是第一列。

# 3D 张量
    将多个矩阵组合成一个新的数组,可以得到一个 3D 张量,你可以将其直观地理解为数字组成的立方体。
    下面是一个 Numpy 的 3D 张量。
    >>> x = np.random.random((3, 2, 4))
    >>> x
    array([[[0.12988825, 0.77277393, 0.46243809, 0.12377541],
            [0.8033865 , 0.81821495, 0.22307274, 0.52566756]],
           [[0.80553673, 0.79450534, 0.14504729, 0.60237352],
            [0.03084524, 0.12027406, 0.45299607, 0.05332909]],
           [[0.68057101, 0.81951978, 0.65863182, 0.85608584],
            [0.68971186, 0.01891051, 0.17315731, 0.02773519]]])
    >>> x.ndim
    3

# 高维张量
    将多个 3D 张量组合成一个数组,可以创建一个 4D 张量,以此类推。
    深度学习处理的一般是 0D 到 4D 的张量,但处理视频数据时可能会遇到 5D 张量。

# 训练集(training set)

# 测试集(test set）

# 权重(weight)或可训练参数(trainable parameter)
    权重是利用随机梯度下降学到的一个或多个张量,其中包含网络的知识。

# 层(layer)
    神经网络的核心组件是层(layer),它是一种数据处理模块,你可以将它看成数据过滤器。
    进去一些数据,出来的数据变得更加有用。具体来说,层从输入数据中提取表示——我们期望这种表示有助于解决手头的问题。
    大多数深度学习都是将简单的层链接起来,从而实现渐进式的数据蒸馏(data distillation)。
    深度学习模型就像是数据处理的筛子,包含一系列越来越精细的数据过滤器(即层)。
    层是一个数据处理模块,将一个或多个输入张量转换为一个或多个输出张量。有些层是无状态的,但大多数的层是有状态的,即层的权重。

# 密集层(dense layer, Dense 层)
    密集连接(也叫全连接)的神经层。
    密集连接层[densely connected layer,也叫全连接层(fully connected layer)或密集层(dense layer),对应于 Keras 的 Dense 类]来处理。

# 循环层(recurrent layer,比如 Keras 的 LSTM 层)

# 二维卷积层(Keras 的 Conv2D )

# softmax 层

# 编译(compile)

# 拟合(fit)

# 过拟合(overfit)
    过拟合是指机器学习模型在新数据上的性能往往比在训练数据上要差

# 损失( loss )

# 精度( acc )

# 损失函数(loss function)
    网络如何衡量在训练数据上的性能,即网络如何朝着正确的方向前进。
    该函数也叫目标函数(objective function)。
    损失函数的输入是网络预测值与真实目标值(即你希望网络输出的结果),然后计算一个距离值,衡量该网络在这个示例上的效果好坏。
    损失函数(目标函数)——在训练过程中需要将其最小化。它能够衡量当前任务是否已成功完成。

# 优化器(optimizer)
    基于训练数据和损失函数来更新网络的机制。
    利用网络预测值与真实目标值的距离值作为反馈信号来对权重值进行微调,以降低当前示例对应的损失值。
    优化器——决定如何基于损失函数对网络进行更新。它执行的是随机梯度下降(SGD)的某个变体。

# 指标(metric)

# 轮次(epoch)
    在所有训练数据上迭代一次叫作一个轮次(epoch)

# 模型的深度(depth)
    数据模型中包含多少层,这被称为模型的深度(depth)。

# logistic 回归(logistic regression,简称 logreg)
    logreg 是一种分类算法,而不是回归算法。

# 监督学习(supervised learning)
    监督学习是目前最常见的机器学习类型。给定一组样本(通常由人工标注),它可以学会将输入数据映射到已知目标[也叫标注(annotation)]。
    其目标是学习训练输入与训练目标之间的关系。

# 序列生成(sequence generation)
    给定一张图像,预测描述图像的文字。序列生成有时可以被重新表示为一系列分类问题,比如反复预测序列中的单词或标记。

#  语法树预测(syntax tree prediction)
    给定一个句子,预测其分解生成的语法树。

# 目标检测(object detection)
    给定一张图像,在图中特定目标的周围画一个边界框。这个问题也可以表示为分类问题(给定多个候选边界框,对每个框内的目标进行分类)或分类与回归联合问题(用向量回归来预测边界框的坐标)。

# 图像分割(image segmentation)
    给定一张图像,在特定物体上画一个像素级的掩模(mask)

# 降维(dimensionality reduction)

# 聚类(clustering)

# 样本(sample)或输入(input)
    进入模型的数据点。

# 预测(prediction)或输出(output)
    从模型出来的结果。

# 目标(target)
    真实值。对于外部数据源,理想情况下,模型应该能够预测出目标。

# 预测误差(prediction error)或损失值(loss value)
    模型预测与目标之间的距离。

# 类别(class)
    分类问题中供选择的一组标签。例如,对猫狗图像进行分类时,“狗”和“猫”就是两个类别。

# 标签(label)
    分类问题中类别标注的具体例子。比如,如果 1234 号图像被标注为

# 真值(ground-truth)或标注(annotation)
    数据集的所有目标,通常由人工收集。
# 二分类(binary classification)
    一种分类任务,每个输入样本都应被划分到两个互斥的类别中。

# 多分类(multiclass classification)
    一种分类任务,每个输入样本都应被划分到两个以上的类别中,比如手写数字分类。

# 多标签分类(multilabel classification)
    一种分类任务,每个输入样本都可以分配多个标签。
    举个例子,如果一幅图像里可能既有猫又有狗,那么应该同时标注“猫”标签和“狗”标签。每幅图像的标签个数通常是可变的。

# 标量回归(scalar regression)
    目标是连续标量值的任务。预测房价就是一个很好的例子,不同的目标价格形成一个连续的空间。

# 向量回归(vector regression)
    目标是一组连续值(比如一个连续向量)的任务。如果对多个值(比如图像边界框的坐标)进行回归,那就是向量回归。

# 小批量(mini-batch)或批量(batch)
    模型同时处理的一小部分样本(样本数通常为 8~128)。
    样本数通常取 2 的幂,这样便于 GPU 上的内存分配。
    训练时,小批量用来为模型权重计算一次梯度下降更新。

# 特征图(feature map)
    对于包含两个空间轴(高度和宽度)和一个深度轴(也叫通道轴)的 3D 张量,其卷积也叫特征图(feature map)。

# 填充(padding)
    填充(padding)是指在输入高和宽的两侧填充元素(通常是 0 元素)。

# 步幅(stride)
    在卷积神经网络中，卷积窗口从输入数组的最左上方开始,按从左往右、从上往下的顺序,依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅(stride)。

# 范数(norm)
    距离的定义是一个宽泛的概念，只要满足非负、自反、三角不等式就可以称之为距离。
    范数是一种强化了的距离概念，它在定义上比距离多了一条数乘的运算法则。
    有时候为了便于理解，我们可以把范数当作距离来理解。
    范数是把一个事物映射到非负实数，且满足非负性、齐次性、三角不等式，符合以上定义的都可以称之为范数
    范数包括向量范数和矩阵范数
    不同的范数表示不同的度量方法，就好比米和光年都可以来度量远近一样；
    
# 向量范数
    向量范数表征向量空间中向量的大小.
    一种非严密的解释就是，对应向量范数，向量空间中的向量都是有大小的，这个大小如何度量，就是用范数来度量的，
    不同的范数都可以来度量这个大小，就好比米和光年都可以来度量远近一样；

    L0范数： L0范数表示向量中非零元素的个数。
    1-范数：，即向量元素绝对值之和; np.linalg.norm([1,2,3], ord=1)
    2-范数：，Euclid范数（欧几里得范数，常用计算向量长度），即向量元素绝对值的平方和再开方;
                np.linalg.norm([1,2,3], ord=2)
                Out[13]: 3.7416573867739413
                math.pow(1*1+2*2+3*3, 0.5)
                Out[9]: 3.7416573867739413
    ∞-范数：，即所有向量元素绝对值中的最大值
                np.linalg.norm([1,2,3], ord=np.inf)
                Out[16]: 3.0
    -∞-范数：，即所有向量元素绝对值中的最小值。
                np.linalg.norm([1,2,3], ord=-np.inf)
                Out[17]: 1.0
    p-范数：，即向量元素绝对值的p次方和的1/p次幂。
    
# 矩阵特征值；特征向量
    设A是n阶方阵，如果数λ和n维非零列向量x使关系式Ax=λx成立，那么这样的数λ称为矩阵A特征值，非零向量x称为A的对应于特征值λ的特征向量。
    式Ax=λx也可写成( A-λE)X=0。这是n个未知数n个方程的齐次线性方程组，它有非零解的充分必要条件是系数行列式| A-λE|=0。
    np.linalg.eig(np.array([[1, 0, 0],
       [0, 5, 0],
       [0, 0, 9]]))
    Out[43]: 
    (array([1., 5., 9.]), array([[1., 0., 0.],
            [0., 1., 0.],
            [0., 0., 1.]]))

# 矩阵范数
    矩阵范数表征矩阵引起变化的大小。
    对于矩阵范数，通过运算AX=BAX=B，可以将向量X变化为B，矩阵范数就是来度量这个变化大小的。
    不同的范数都可以来度量这个大小，就好比米和尺都可以来度量远近一样；
    一个集合（向量），通过一种映射关系（矩阵），得到另外一个集合（另外一个向量）。
    矩阵的范数，就是表示这个变化过程的大小的一个度量。矩阵范数反映了线性映射把一个向量映射为另一个向量，向量的“长度”缩放的比例。
    1-范数：， 列和范数，即所有矩阵列向量绝对值之和的最大值，
                np.linalg.norm([[1,2,3], [4, 5, 6]], ord=1)
                Out[19]: 9.0
    2-范数：，谱范数，即AT·A矩阵的最大特征值的开平方。
                np.linalg.norm([[1,2,3], [4, 5, 6]], ord=2)
                Out[31]: 9.508032000695724
                x = np.array([[1,2,3], [4, 5, 6]])
                x.T
                Out[36]: 
                array([[1, 4],
                       [2, 5],
                       [3, 6]])
                np.dot(x.T, x)
                Out[37]: 
                array([[17, 22, 27],
                       [22, 29, 36],
                       [27, 36, 45]])
                # 计算特征值及特征向量       
                np.linalg.eig(np.dot(x.T, x))
                Out[38]: 
                (array([9.04026725e+01, 5.97327474e-01, 7.23299057e-16]),
                 array([[-0.42866713, -0.80596391,  0.40824829],
                        [-0.56630692, -0.11238241, -0.81649658],
                        [-0.7039467 ,  0.58119908,  0.40824829]]))
                math.pow(max(np.linalg.eig(np.dot(x.T, x))[0]), 0.5)
                Out[39]: 9.508032000695724

    ∞-范数：，行和范数，即所有矩阵行向量绝对值之和的最大值。
    F-范数：，Frobenius范数，即矩阵元素绝对值的平方和再开平方。
    
# 向量模长
    向量模长； 即向量元素绝对值的平方和再开方;
    向量的第二范数为传统意义上的向量长度
    向量的模，sum(vector**2)**0.5
    math.sqrt(sum(vec**2 for vec in vector))
    
# 矩阵乘积
    矩阵乘积; 矩阵相乘最重要的方法是一般矩阵乘积。
    它只有在第一个矩阵的列数（column）和第二个矩阵的行数（row）相同时才有意义  。
    np.dot([[1,2,3], [4, 5,6]], [[1,2], [3,4], [5,6]])
    Out[63]: 
    array([[22, 28],
           [49, 64]])

# 点乘运算
    就是对这两个向量对应位一一相乘之后求和的操作; 即：若a=(x1,y1),b=(x2,y2)，则a·b=x1·x2+y1·y2
    np.dot([1,2,3], [4,5,6])
    Out[4]: 32
    1*4 + 2*5 + 3*6
    Out[5]: 32
    
# np.dot
    1.对于一维数组，其作用同inner
    np.dot([1,2], [3,4])
    Out[75]: 11
    np.inner([1,2], [3,4])
    Out[76]: 11

    2.对于二维数组，其作用是矩阵乘法
    np.dot([[1,2,3], [4,5,6]], [[1,0,0], [1,2,3], [4,5,6]])
    Out[69]: 
    array([[15, 19, 24],
           [33, 40, 51]])
 
    对于多维数组，其作用是将数组a的最后轴上的所有元素与数组b的倒数第二轴上的所有元素的乘积和
    np.dot([[    [0, 2, 0],
                 [1,2,3]],
                [[1,2,3], 
                 [4,5,6]] ], 
            [[[1,0,0], 
              [1,1,1], 
              [0, 0, 0]],
            [[1,0,0], 
             [1,2,3], 
             [4,5,6]]])
    Out[80]: 
    array([[[[ 2,  2,  2],
             [ 2,  4,  6]],
            [[ 3,  2,  2],
             [15, 19, 24]]],
           [[[ 3,  2,  2],
             [15, 19, 24]],
            [[ 9,  5,  5],
             [33, 40, 51]]]])

# 点积
    点积，又叫点乘，向量内积、数量积,dot product; scalar product,标量积
    公式：a * b = |a| * |b| * cosθ = a1*b1 + a2*b2 + a3*b3 + ... + an*bn 
    这里要求一维向量a和向量b的行列数相同。
    点乘又叫向量的内积、数量积，是一个向量和它在另一个向量上的投影的长度的乘积；是标量。 
    点乘反映着两个向量的“相似度”，两个向量越“相似”，它们的点乘越大。
    对两个向量执行点乘运算，就是对这两个向量对应位一一相乘之后求和的操作，点乘的结果是一个标量。
    向量内积,向量a和b的长度之积再乘以它们之间的夹角的余弦；向量内积的几何解释就是一个向量在另一个向量上的投影的积
    内积指的是一个向量(在另一个向量上)的投影乘上另一个向量的模(可以理解为向量的长度)，如果内积为零，意思是互相之间没有投影。
    np.inner([1, 0], [1, 2])
    Out[52]: 1
    math.sqrt(sum(vec**2 for vec in [1, 0])) 
    Out[53]: 1.0
    math.sqrt(sum(vec**2 for vec in [1, 2])) 
    Out[54]: 2.23606797749979
    对于一维数组，np.dot 其作用同np.inner

    # 对于两个二维数组的inner，相当于按X和Y的最后顺序的轴方向上取向量，
    # 然后依次计算内积后组成的多维数组
    np.inner([[1,2,3], [4,5,6]], [[1,0,0], [1,2,3], [4,5,6]])
    Out[67]: 
    array([[ 1, 14, 32],
           [ 4, 32, 77]])
    
    [[np.dot([1,2,3], [1,0,0]), np.dot([1,2,3],  [1,2,3]), np.dot([1,2,3], [4,5,6])], 
    [np.dot([4,5,6], [1,0,0]), np.dot([4,5,6], [1,2,3]), np.dot([4,5,6], [4,5,6])]]
    Out[71]: [[1, 14, 32], 
              [4, 32, 77]]


# 正交基(Orthogonal Basis)
    三个向量两两之间互相的内积等于零，于是这三个向量就是一组简单的正交基。

# 标准正交基
    每组正交基中的向量，其模(可以认为是长度)的大小都是1，这样的情况称为标准正交基。

# 叉乘
    两个向量的叉乘，又叫向量积、外积、叉积，叉乘的运算结果是一个向量而不是一个标量。并且两个向量的叉积与这两个向量组成的坐标平面垂直。
    向量的叉乘，即求同时垂直两个向量的向量，即c垂直于a，同时c垂直于b（a与c的夹角为90°，b与c的夹角为90°）
    c =  a×b = （a.y*b.z-b.y*a.z , b.x*a.z-a.x*b.z  , a.x*b.y-b.x*a.y）
    在二维空间中，叉乘还有另外一个几何意义就是：aXb等于由向量a和向量b构成的平行四边形的面积。
    在三维几何中，向量a和向量b的叉乘结果是一个向量，更为熟知的叫法是法向量，该向量垂直于a和b向量构成的平面。
    两个向量的外积，又叫叉乘、叉积向量积，其运算结果是一个向量而不是一个标量。并且两个向量的外积与这两个向量组成的坐标平面垂直。
    定义：向量a与b的外积a×b是一个向量，其长度等于|a×b| = |a||b|sin∠(a,b)，其方向正交于a与b。并且，(a,b,a×b)构成右手系。 
    特别地，0×a = a×0 = 0.此外，对任意向量a，a×a=0。
    向量的叉乘：a ∧ b
    a ∧ b = |a| * |b| * sinθ 
    向量积被定义为： 
    模长：（在这里θ表示两向量之间的夹角(共起点的前提下)（0° ≤ θ ≤ 180°），它位于这两个矢量所定义的平面上。） 
    方向：a向量与b向量的向量积的方向与这两个向量所在平面垂直，且遵守右手定则。
    若坐标系是满足右手定则的，当右手的四指从a以不超过180度的转角转向b时，竖起的大拇指指向是c的方向。c = a ∧ b
    np.outer只对一维数组进行计算，如果传入的是多维数组，则先将此数组展平为一维数组之后再进行算。
    outer计算列向量和行向量的矩阵乘积，即结果为矩阵
    np.outer([1,2,3], [4,5,6])
    Out[72]: 
    array([[ 4,  5,  6],
           [ 8, 10, 12],
           [12, 15, 18]])
           
    [row * np.array([4,5,6]) for row in [1,2,3]]
    Out[74]: [array([4, 5, 6]), array([ 8, 10, 12]), array([12, 15, 18])]

# 张量积np.tensordot
    tensordot()将两个多维数组a和b指定轴上的对应元素相乘并求和，它是最一般化的乘积运算函数。
    np.tensordot(Z,X,axes=[[0],[1]]) #指定Z的0轴与X的1轴进行乘积求和
    a = np.arange(60.).reshape(3,4,5)
    b = np.arange(24.).reshape(4,3,2)
    c = np.tensordot(a,b, axes=([1,0],[0,1]))

    对于多维数组的dot乘积，相当于tensordot(a,b,axes=[[-1],[-2]])
    np.dot(X,Z) == np.tensordot(X,Z,axes=[[-1],[-2]])

# feature map 
    应该是特征图的意思，是指每个卷积核和输入卷积后形成的特征图，特征图的个数和卷积核的个数相同

# 卷积核
    
# 互相关（cross-correlation）
    互相关
    设两个函数分别是f(t)和g(t)，则互相关函数定义为：
    它反映的是两个函数在不同的相对位置上互相匹配的程度。
    对卷积不要求倒序操作，也就是互相关；互相关不满足交换律；

# 卷积
    卷积，就是信号B(数据B)与信号A(数据A)错开时间或空间的内积和，错开的时间或空间长度就是卷积结果的自变量。
    或者说，离散域的卷积就是数据A与数据B里面的一段数据逐个相乘，然后再加起来。
    这里的“一段”就是卷积核的长度，当然也可以是无限长。连续域就是把加起来换成积分。
    卷积的方式有三种：
    如果自始至终卷积核都在“信号内”，则最后得到的结果长度会小于待卷积信号长度。假设待卷积信号的长度是n,卷积核大小是m,则结果长度为n-m+1;这种卷积的方式称为 valid;
    如果卷积核的中心刚好从待卷积信号的第一个元素滑到最后一个元素，则需要把原来的信号扩展长度；一般来说扩展的方式是在原来信号的边缘添加0元素，这个过程通常称为零填充(zero padding);通过零填充，卷积结果的长度和待卷积信号长度一样，这种卷积的方式称为same;
    如果通过零填充把卷积核能够划过的位置扩展到最大，则结果长度为 n+m-1;这种方式称为full.
    
    
    

