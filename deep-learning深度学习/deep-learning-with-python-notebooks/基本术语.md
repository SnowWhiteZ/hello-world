
# 类(class)
    在机器学习中,分类问题中的某个类别叫作类(class)。

# 样本(sample)
    数据点叫作样本(sample)。

# 标签(label)
    某个样本对应的类叫作标签(label)。

# 张量(tensor)
    是一个数据容器。它包含的数据几乎总是数值数据,因此它是数字的容器。你可能对矩阵很熟悉,它是二维张量。
    张量是矩阵向任意维度的推广[注意,张量的维度(dimension)通常叫作轴(axis)]。

# 维度(dimension)
    可以表示沿着某个轴上的元素个数(比如 5D 向量),也可以表示张量中轴的个数(比如 5D 张量),这有时会令人感到混乱。
    对于后一种情况,技术上更准确的说法是 5 阶张量(张量的阶数即轴的个数),但 5D 张量这种模糊的写法更常见。

# 轴(axis)

# 样本轴
    通常来说,深度学习中所有数据张量的第一个轴(0 轴,因为索引从 0 开始)都是样本轴(samples axis,有时也叫样本维度)

# 批量
    深度学习模型不会同时处理整个数据集,而是将数据拆分成小批量。
    具体来看,下面是 MNIST 数据集的一个批量,批量大小为 128。
    batch = train_images[:128]
    然后是下一个批量。
    batch = train_images[128:256]
    然后是第 n 个批量。
    batch = train_images[128 * n:128 * (n + 1)]
    对于这种批量张量,第一个轴(0 轴)叫作批量轴(batch axis)或批量维度(batch dimension)。

# 向量数据
    2D 张量,形状为 (samples, features) 。
    对于这种数据集,每个数据点都被编码为一个向量,因此一个数据批量就被编码为 2D 张量(即向量组成的数组),其中第一个轴是样本轴,第二个轴是特征轴。

# 时间序列数据或序列数据
    3D 张量,形状为 (samples, timesteps, features) 。
    当时间(或序列顺序)对于数据很重要时,应该将数据存储在带有时间轴的 3D 张量中。
    每个样本可以被编码为一个向量序列(即 2D 张量),因此一个数据批量就被编码为一个 3D 张量。
    根据惯例,时间轴始终是第 2 个轴(索引为 1 的轴)。

# 图像
    4D 张量,形状为 (samples, height, width, channels) 或 (samples, channels,height, width) 。
    图像通常具有三个维度:高度、宽度和颜色深度。
    虽然灰度图像(比如 MNIST 数字图像)只有一个颜色通道,因此可以保存在 2D 张量中,但按照惯例,图像张量始终都是 3D 张量,灰度图像的彩色通道只有一维。
    因此,如果图像大小为 256×256,那么 128 张灰度图像组成的批量可以保存在一个形状为 (128, 256, 256, 1) 的张量中,
    而 128 张彩色图像组成的批量则可以保存在一个形状为 (128, 256, 256, 3) 的张量中。
    图像张量的形状有两种约定:通道在后(channels-last)的约定(在 TensorFlow 中使用)和通道在前(channels-first)的约定(在 Theano 中使用)。
    Google 的 TensorFlow 机器学习框架将颜色深度轴放在最后: (samples, height, width, color_depth) 。
    与此相反,Theano将图像深度轴放在批量轴之后: (samples, color_depth, height, width) 。
    如果采用 Theano 约定,前面的两个例子将变成 (128, 1, 256, 256) 和 (128, 3, 256, 256) 。
    Keras 框架同时支持这两种格式。

# 视频
    5D 张量,形状为 (samples, frames, height, width, channels) 或 (samples,frames, channels, height, width) 。
    视频数据是现实生活中需要用到 5D 张量的少数数据类型之一。视频可以看作一系列帧,每一帧都是一张彩色图像。
    由于每一帧都可以保存在一个形状为 (height, width, color_depth) 的 3D 张量中,
    因此一系列帧可以保存在一个形状为 (frames, height, width, color_depth) 的 4D 张量中,
    而不同视频组成的批量则可以保存在一个 5D 张量中,其形状为(samples, frames, height, width, color_depth) 。

# 广播(broadcast)
    将一个 2D 张量与一个向量相加。如果将两个形状不同的张量相加,如果没有歧义的话,较小的张量会被广播(broadcast),以匹配较大张量的形状。
    广播包含以下两步。
    (1) 向较小的张量添加轴(叫作广播轴),使其 ndim 与较大的张量相同。
    (2) 将较小的张量沿着新轴重复,使其形状与较大的张量相同。

# 张量点积
    点积运算,也叫张量积(tensor product,不要与逐元素的乘积弄混),是最常见也最有用的张量运算。
    与逐元素的运算不同,它将输入张量的元素合并在一起。
    注意,两个向量之间的点积是一个标量,而且只有元素个数相同的向量之间才能做点积。
    对一个矩阵 x 和一个向量 y 做点积,返回值是一个向量,其中每个元素是 y 和 x的每一行之间的点积。

# 张量变形(tensor reshaping)
    张量变形是指改变张量的行和列,以得到想要的形状。变形后的张量的元素总个数与初始张量相同。
    简单的例子可以帮助我们理解张量变形。
    >>> x = np.array([[0., 1.],
                                    [2., 3.],
                                    [4., 5.]])
    >>> print(x.shape)
    (3, 2)
    >>> x = x.reshape((6, 1))
    >>> x
    array([[ 0.],
    [ 1.],
    [ 2.],
    [ 3.],
    [ 4.],
    [ 5.]])
    >>> x = x.reshape((2, 3))
    >>> x
    array([[ 0., 1., 2.],
                [ 3., 4., 5.]])

# 阶(rank)
    张量轴的个数也叫作阶(rank)。
    例如,3D 张量有 3 个轴,矩阵有 2 个轴。这在 Numpy 等 Python 库中也叫张量的 ndim 。
    >>> x = np.random.random((3, 2))
    >>> x
    array([[0.8957804 , 0.96712402],
           [0.73855227, 0.34115149],
           [0.36667734, 0.16249242]])
    >>> x.ndim
    2

# 形状
    这是一个整数元组,表示张量沿每个轴的维度大小(元素个数)。
    例如,前面矩阵示例的形状为 (3, 5) ,3D 张量示例的形状为 (3, 3, 5) 。
    向量的形状只包含一个元素,比如 (5,) ,而标量的形状为空,即 () 。
    >>> x = np.array(3)
    >>> x.shape
    ()
    >>> x = np.array([2,3])
    >>> x.shape
    (2,)

# 数据类型(在 Python 库中通常叫作 dtype )
    这是张量中所包含数据的类型,例如,张量的类型可以是 float32 、 uint8 、 float64 等。
    在极少数情况下,你可能会遇到字符( char )张量。
    注意,Numpy(以及大多数其他库)中不存在字符串张量,因为张量存储在预先分配的连续内存段中,而字符串的长度是可变的,无法用这种方式存储。
    >>> x = np.array([2.0,3])
    >>> x.dtype
    dtype('float64')

# 标量(scalar, 0D 张量)
    仅包含一个数字的张量叫作标量(scalar,也叫标量张量、零维张量、0D 张量)。
    在 Numpy中,一个 float32 或 float64 的数字就是一个标量张量(或标量数组)。
    你可以用 ndim 属性来查看一个 Numpy 张量的轴的个数。标量张量有 0 个轴( ndim == 0 )。
    张量轴的个数也叫作阶(rank)。下面是一个 Numpy 标量。
    >>> import numpy as np
    >>> x = np.array(12)
    >>> x
    array(12)
    >>> x.ndim
    0

# 向量(vector, 1D 张量)
    数字组成的数组叫作向量(vector)或一维张量(1D 张量)。一维张量只有一个轴。
    下面是一个 Numpy 向量。
    >>> x = np.array([12, 3, 6, 14, 7])
    >>> x
    array([12, 3, 6, 14, 7])
    >>> x.ndim
    1

    这个向量有 5 个元素,所以被称为 5D 向量。不要把 5D 向量和 5D 张量弄混!
    5D 向量只有一个轴,沿着轴有 5 个维度,而 5D 张量有 5 个轴(沿着每个轴可能有任意个维度)。

# 矩阵(matrix, 2D 张量)
    向量组成的数组叫作矩阵(matrix)或二维张量(2D 张量)。
    矩阵有 2 个轴(通常叫作行和列)。你可以将矩阵直观地理解为数字组成的矩形网格。
    下面是一个 Numpy 矩阵。
    >>> x = np.array([[5, 78, 2, 34, 0],
                                    [6, 79, 3, 35, 1],
                                    [7, 80, 4, 36, 2]])
    >>> x.ndim
    2
    第一个轴上的元素叫作行(row),第二个轴上的元素叫作列(column)。
    在上面的例子中,[5, 78, 2, 34, 0] 是 x 的第一行, [5, 6, 7] 是第一列。

# 3D 张量
    将多个矩阵组合成一个新的数组,可以得到一个 3D 张量,你可以将其直观地理解为数字组成的立方体。
    下面是一个 Numpy 的 3D 张量。
    >>> x = np.random.random((3, 2, 4))
    >>> x
    array([[[0.12988825, 0.77277393, 0.46243809, 0.12377541],
            [0.8033865 , 0.81821495, 0.22307274, 0.52566756]],
           [[0.80553673, 0.79450534, 0.14504729, 0.60237352],
            [0.03084524, 0.12027406, 0.45299607, 0.05332909]],
           [[0.68057101, 0.81951978, 0.65863182, 0.85608584],
            [0.68971186, 0.01891051, 0.17315731, 0.02773519]]])
    >>> x.ndim
    3

# 高维张量
    将多个 3D 张量组合成一个数组,可以创建一个 4D 张量,以此类推。
    深度学习处理的一般是 0D 到 4D 的张量,但处理视频数据时可能会遇到 5D 张量。

# 训练集(training set)

# 测试集(test set）

# 权重(weight)或可训练参数(trainable parameter)
    权重是利用随机梯度下降学到的一个或多个张量,其中包含网络的知识。

# 层(layer)
    神经网络的核心组件是层(layer),它是一种数据处理模块,你可以将它看成数据过滤器。
    进去一些数据,出来的数据变得更加有用。具体来说,层从输入数据中提取表示——我们期望这种表示有助于解决手头的问题。
    大多数深度学习都是将简单的层链接起来,从而实现渐进式的数据蒸馏(data distillation)。
    深度学习模型就像是数据处理的筛子,包含一系列越来越精细的数据过滤器(即层)。
    层是一个数据处理模块,将一个或多个输入张量转换为一个或多个输出张量。有些层是无状态的,但大多数的层是有状态的,即层的权重。

# 密集层(dense layer, Dense 层)
    密集连接(也叫全连接)的神经层。
    密集连接层[densely connected layer,也叫全连接层(fully connected layer)或密集层(dense layer),对应于 Keras 的 Dense 类]来处理。

# 循环层(recurrent layer,比如 Keras 的 LSTM 层)

# 二维卷积层(Keras 的 Conv2D )

# softmax 层

# 编译(compile)

# 拟合(fit)

# 过拟合(overfit)
    过拟合是指机器学习模型在新数据上的性能往往比在训练数据上要差

# 损失( loss )

# 精度( acc )

# 损失函数(loss function)
    网络如何衡量在训练数据上的性能,即网络如何朝着正确的方向前进。
    该函数也叫目标函数(objective function)。
    损失函数的输入是网络预测值与真实目标值(即你希望网络输出的结果),然后计算一个距离值,衡量该网络在这个示例上的效果好坏。
    损失函数(目标函数)——在训练过程中需要将其最小化。它能够衡量当前任务是否已成功完成。

# 优化器(optimizer)
    基于训练数据和损失函数来更新网络的机制。
    利用网络预测值与真实目标值的距离值作为反馈信号来对权重值进行微调,以降低当前示例对应的损失值。
    优化器——决定如何基于损失函数对网络进行更新。它执行的是随机梯度下降(SGD)的某个变体。

# 指标(metric)

# 轮次(epoch)
    在所有训练数据上迭代一次叫作一个轮次(epoch)

# 模型的深度(depth)
    数据模型中包含多少层,这被称为模型的深度(depth)。

# logistic 回归(logistic regression,简称 logreg)
    logreg 是一种分类算法,而不是回归算法。

# 监督学习(supervised learning)
    监督学习是目前最常见的机器学习类型。给定一组样本(通常由人工标注),它可以学会将输入数据映射到已知目标[也叫标注(annotation)]。
    其目标是学习训练输入与训练目标之间的关系。

# 序列生成(sequence generation)
    给定一张图像,预测描述图像的文字。序列生成有时可以被重新表示为一系列分类问题,比如反复预测序列中的单词或标记。

#  语法树预测(syntax tree prediction)
    给定一个句子,预测其分解生成的语法树。

# 目标检测(object detection)
    给定一张图像,在图中特定目标的周围画一个边界框。这个问题也可以表示为分类问题(给定多个候选边界框,对每个框内的目标进行分类)或分类与回归联合问题(用向量回归来预测边界框的坐标)。

# 图像分割(image segmentation)
    给定一张图像,在特定物体上画一个像素级的掩模(mask)

# 降维(dimensionality reduction)

# 聚类(clustering)

# 样本(sample)或输入(input)
    进入模型的数据点。

# 预测(prediction)或输出(output)
    从模型出来的结果。

# 目标(target)
    真实值。对于外部数据源,理想情况下,模型应该能够预测出目标。

# 预测误差(prediction error)或损失值(loss value)
    模型预测与目标之间的距离。

# 类别(class)
    分类问题中供选择的一组标签。例如,对猫狗图像进行分类时,“狗”和“猫”就是两个类别。

# 标签(label)
    分类问题中类别标注的具体例子。比如,如果 1234 号图像被标注为

# 真值(ground-truth)或标注(annotation)
    数据集的所有目标,通常由人工收集。
# 二分类(binary classification)
    一种分类任务,每个输入样本都应被划分到两个互斥的类别中。

# 多分类(multiclass classification)
    一种分类任务,每个输入样本都应被划分到两个以上的类别中,比如手写数字分类。

# 多标签分类(multilabel classification)
    一种分类任务,每个输入样本都可以分配多个标签。
    举个例子,如果一幅图像里可能既有猫又有狗,那么应该同时标注“猫”标签和“狗”标签。每幅图像的标签个数通常是可变的。

# 标量回归(scalar regression)
    目标是连续标量值的任务。预测房价就是一个很好的例子,不同的目标价格形成一个连续的空间。

# 向量回归(vector regression)
    目标是一组连续值(比如一个连续向量)的任务。如果对多个值(比如图像边界框的坐标)进行回归,那就是向量回归。

# 小批量(mini-batch)或批量(batch)
    模型同时处理的一小部分样本(样本数通常为 8~128)。
    样本数通常取 2 的幂,这样便于 GPU 上的内存分配。
    训练时,小批量用来为模型权重计算一次梯度下降更新。

# 特征图(feature map)
    对于包含两个空间轴(高度和宽度)和一个深度轴(也叫通道轴)的 3D 张量,其卷积也叫特征图(feature map)。

# 填充(padding)
    填充(padding)是指在输入高和宽的两侧填充元素(通常是 0 元素)。

# 步幅(stride)
    在卷积神经网络中，卷积窗口从输入数组的最左上方开始,按从左往右、从上往下的顺序,依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅(stride)。








