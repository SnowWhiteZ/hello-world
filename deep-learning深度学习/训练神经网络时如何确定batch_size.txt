
训练神经网络时如何确定batch size

神经网络的一次迭代过程：
首先选择n个样本组成一个batch，然后将batch丢进神经网络，得到输出结果。
再将输出结果与样本label丢给loss函数算出本轮的loss，而后就从后往前逐层计算参数之于loss的导数。
最后将每个参数的导数配合步长参数来进行参数更新。
这就是训练过程的一次迭代。

batch_size: 超参数就是batch的大小
方法1：我们可以一次性将整个数据集喂给神经网络，让神经网络利用全部样本来计算迭代时的梯度（即传统的梯度下降法） batch_size = len(train_data)
方法2：也可以一次只喂一个样本（即随机梯度下降法，也称在线梯度下降法） batch_size = 1
方法3：也可以取个折中的方案，即每次喂一部分样本让其完成本轮迭代（即batch梯度下降法）1 < batch_size < len(train_data)

方法1与方法2的区别（假设我们总共有500个样本）：
方法1：
total = 旧参下计算更新值1+旧参下计算更新值2+...+旧参下计算更新值500 ;
新参数 = 旧参数 + total

方法2：
新参数1 = 旧参数 + 旧参数下计算更新值1；
新参数2 = 新参数1 + 新参数1下计算更新值1；
新参数3 = 新参数2 + 新参数2下计算更新值1；
...
新参数500 = 新参数500 + 新参数500下计算更新值1；

也就是说，方法1是将参数一次性更新500个样本的量，方法2是迭代的更新500次参数。

方法1与方法2的收敛速度比较：
假设每个样本相对于真实分布的标准差为σ，那么n个样本的标准差为 σ / √n 。
使用样本来估计梯度的时候，1个样本带来σ的标准差，但是使用n个样本区估计梯度并不能让标准差线性降低（也就是并不能让误差降低为原来的1/n，即无法达到σ/n），
而n个样本的计算量却是线性的（每个样本都要平等的跑一遍前向算法）。
故在同等的计算量之下（一定的时间内），使用整个样本集的收敛速度要远慢于使用少量样本的情况。
换句话说，要想收敛到同一个最优点，使用整个样本集时，虽然迭代次数少，但是每次迭代的时间长，耗费的总时间是大于使用少量样本多次迭代的情况的。

使用单个单核cpu的情况下，样本越少（batch_size越小），收敛越快；
使用GPU训练时，跑一个样本花的时间与跑几十个样本甚至几百个样本的时间是一样的！当然得益于GPU里面超多的核，超强的并行计算能力。
因此，在工程实际中，从收敛速度的角度来说，小批量的样本集是最优的，也就是我们所说的mini-batch。
这时的batch size往往从几十到几百不等，但一般不会超过几千（有土豪显卡除外）。

假设有非常强的显卡，使得一次计算10000个样本跟计算1个样本的时间相同的话，设置batch_size=10000从收敛速度上来说是最好的; 但这样设置往往容易导入陷入局部最优点。
传统的最优化算法是无法自动的避开局部最优点的，对于鞍点也是理论上很头疼的东西。
样本量少（batch_size设置较小）的时候会带来很大的方差，而这个大方差恰好会导致我们在梯度下降到很差的局部最优点（只是微微凸下去的最优点）和鞍点的时候不稳定，
一不小心就因为一个大噪声的到来导致跳了局部最优点，或者跳出鞍点，从而有机会去寻找全局最优点。
与之相反的，当样本量很多（batch_size设置较大）时，方差很小，对梯度的估计要准确和稳定的多，
因此反而在差劲的局部最优点和鞍点时反而容易自信的呆着不走了，从而导致神经网络收敛到很差的点上。

以上讨论是基于梯度下降的，而且默认是一阶的（即没有利用二阶导数信息，仅仅使用一阶导数去优化）。
因此对于SGD（随机梯度下降）及其改良的一阶优化算法如Adagrad、Adam等是没问题的，但是对于强大的二阶优化算法如共轭梯度法、L-BFGS来说，如果估计不好一阶导数，那么对二阶导数的估计会有更大的误差，这对于这些算法来说是致命的。
因此，对于二阶优化算法，减小batch换来的收敛速度提升远不如引入大量噪声导致的性能下降，因此在使用二阶优化算法时，往往要采用大batch。此时往往batch设置成几千甚至一两万才能发挥出最佳性能。

资料来源：https://www.jiqizhixin.com/articles/2018-07-12-4






