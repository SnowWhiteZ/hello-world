自回归是时间序列分析或者信号处理领域喜欢用的一个术语，我们这里理解成语言模型就好了。一个句子的生成过程如下：首先根据概率分布生成第一个词，然后根据第一个词生成第二个词，然后根据前两个词生成第三个词了，以此类推，直到生成整个句子。

所谓的自编码器是一种无监督学习输入的特征的方法：我们用一个神经网络把输入(输入通常还会增加一些噪声)变成一个低维的特征，这就是编码部分。然后再用一个Decoder尝试把特征恢复成原始的信号。例如：可以把Bert看成一种AutoEncoder，它通过Mask改变了部分Token，然后试图通过其上下文的其它Token来恢复这些被Mask的Token。

BERT 预训练方式属于自编码AE (Auto Encoder)。原因在于 BERT 预训练目标为根据上下文语境复原被 [MASK]随机遮挡的文字。此过程与降噪自编码机的原理一致，属于自编码模型 。自编码模型的缺陷是 [MASK] 标记出现在预训练中，却未出现在下流任务的微调中（比如阅读理解，文本分类等任务）。可能出现预训练与微调任务目标不一致的问题。另外，BERT对多个 [MASK] 的预测互相独立。

GPT 或 LSTM 的训练方式属于自回归 AR (Auto Regressive)。即一个语句出现的概率可以因式分解为一长串条件概率的乘积。自回归模型的缺陷是要么从前往后做自回归，要么从后往前做自回归，没有像 BERT 一样使用双向编码。因为没有使用 [MASK], 所以解决了BERT 预训练与微调目标不匹配的问题。




