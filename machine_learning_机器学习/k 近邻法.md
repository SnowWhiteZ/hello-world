# k 近邻法

 k近邻法（k-Nearest Neighbor:kNN）是一种基本的分类与回归方法


分类问题：对新的样本，根据其 k个最近邻的训练样本的类别，
通过多数表决等方式进行预测。

回归问题：对新的样本，根据其 k个最近邻的训练样本标签值的均值作为预测值。


该算法计算步骤如下： 
(1)算距离：给定的输入实例，计算它与训练集中所有样本点的距离； 
(2)找近邻：划定与输入实例最邻近的k个样本点，作为测试近邻； 
(3)投票表决：少数服从多数。把测试样本中实例最多的类别作为输入实例的分类输出。


如果k比较小，就相当于算法划定的近邻范围比较小，此时只有与输入实例非常相似的实例才会对预测结果产生影响，近似误差会减小，但缺点就是估计误差会增大，就是说，万一近邻的实例是噪声，预测就会出错。简单地说，k值减小就意味着整体模型变得复杂，容易发生过拟合。

如果k值较大，可以减少估计误差，但缺点就是近似误差会增大，此时与输入实例距离较远的实例也会对预测产生影响，k值增大就意味着整体的模型变得简单。

k值选择反映了对近似误差与估计误差之间的权衡，通常由交叉验证选择最优的k


近似误差：可以理解为对现有训练集的训练误差。 
估计误差：可以理解为对测试集的测试误差。

近似误差关注训练集，如果近似误差小了会出现过拟合的现象，对现有的训练集能有很好的预测，但是对未知的测试样本将会出现较大偏差的预测。模型本身不是最接近最佳模型。

估计误差关注测试集，估计误差小了说明对未知数据的预测能力好。模型本身最接近最佳模型。


NN与KNN区别：
      NN：我们找训练数据中最相似的1张图片的标签来作为测试图像的标签。

      KNN：我们找训练数据中最相似的k个图片的标签，然后让他们针对测试图片进行投票，最后把票数最高的标签作为对测试图片的预测。

      所以当k=1的时候，k-Nearest Neighbor分类器就是Nearest Neighbor分类器。

加权KNN

让距离越近的点对于决策越重要。
比较 最近的K个点的距离d 的单调递减函数f(d)的函数值


懒惰学习：在训练模型过程中这类算法并不去学习一个判别式函数(损失函数)而是在训练阶段仅仅是把样本保存起来，训练时间开销为零，待收到测试样本后再进行处理。
急切学习:在训练阶段就对样本进行学习处理的模型

